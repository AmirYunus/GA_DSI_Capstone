{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To run this template just do:\n",
    "python gan.py\n",
    "After a few epochs, launch tensorboard to see the images being generated at every batch.\n",
    "tensorboard --logdir default\n",
    "\"\"\"\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, img_shape):\n",
    "        super(Generator, self).__init__()\n",
    "        self.img_shape = img_shape\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *self.img_shape)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "\n",
    "        return validity\n",
    "\n",
    "\n",
    "class GAN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super(GAN, self).__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        # networks\n",
    "        mnist_shape = (1, 28, 28)\n",
    "        self.generator = Generator(latent_dim=hparams.latent_dim, img_shape=mnist_shape)\n",
    "        self.discriminator = Discriminator(img_shape=mnist_shape)\n",
    "\n",
    "        # cache for generated images\n",
    "        self.generated_imgs = None\n",
    "        self.last_imgs = None\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return F.binary_cross_entropy(y_hat, y)\n",
    "\n",
    "    def training_step(self, batch, batch_nb, optimizer_i):\n",
    "        imgs, _ = batch\n",
    "        self.last_imgs = imgs\n",
    "\n",
    "        # train generator\n",
    "        if optimizer_i == 0:\n",
    "            # sample noise\n",
    "            z = torch.randn(imgs.shape[0], self.hparams.latent_dim)\n",
    "\n",
    "            # match gpu device (or keep as cpu)\n",
    "            if self.on_gpu:\n",
    "                z = z.cuda(imgs.device.index)\n",
    "\n",
    "            # generate images\n",
    "            self.generated_imgs = self.forward(z)\n",
    "\n",
    "            # log sampled images\n",
    "            # sample_imgs = self.generated_imgs[:6]\n",
    "            # grid = torchvision.utils.make_grid(sample_imgs)\n",
    "            # self.logger.experiment.add_image('generated_images', grid, 0)\n",
    "\n",
    "            # ground truth result (ie: all fake)\n",
    "            valid = torch.ones(imgs.size(0), 1)\n",
    "\n",
    "            # adversarial loss is binary cross-entropy\n",
    "            g_loss = self.adversarial_loss(self.discriminator(self.generated_imgs), valid)\n",
    "            tqdm_dict = {'g_loss': g_loss}\n",
    "            output = OrderedDict({\n",
    "                'loss': g_loss,\n",
    "                'progress_bar': tqdm_dict,\n",
    "                'log': tqdm_dict\n",
    "            })\n",
    "            return output\n",
    "\n",
    "        # train discriminator\n",
    "        if optimizer_i == 1:\n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "\n",
    "            # how well can it label as real?\n",
    "            valid = torch.ones(imgs.size(0), 1)\n",
    "            real_loss = self.adversarial_loss(self.discriminator(imgs), valid)\n",
    "\n",
    "            # how well can it label as fake?\n",
    "            fake = torch.zeros(imgs.size(0), 1)\n",
    "            fake_loss = self.adversarial_loss(\n",
    "                self.discriminator(self.generated_imgs.detach()), fake)\n",
    "\n",
    "            # discriminator loss is the average of these\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            tqdm_dict = {'d_loss': d_loss}\n",
    "            output = OrderedDict({\n",
    "                'loss': d_loss,\n",
    "                'progress_bar': tqdm_dict,\n",
    "                'log': tqdm_dict\n",
    "            })\n",
    "            return output\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        b1 = self.hparams.b1\n",
    "        b2 = self.hparams.b2\n",
    "\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        return [opt_g, opt_d], []\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.5], [0.5])])\n",
    "        dataset = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n",
    "        return DataLoader(dataset, batch_size=self.hparams.batch_size)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        z = torch.randn(8, self.hparams.latent_dim)\n",
    "        # match gpu device (or keep as cpu)\n",
    "        if self.on_gpu:\n",
    "            z = z.cuda(self.last_imgs.device.index)\n",
    "\n",
    "        # log sampled images\n",
    "        sample_imgs = self.forward(z)\n",
    "        grid = torchvision.utils.make_grid(sample_imgs)\n",
    "        self.logger.experiment.add_image(f'generated_images', grid, self.current_epoch)\n",
    "\n",
    "\n",
    "def main(hparams):\n",
    "    # ------------------------\n",
    "    # 1 INIT LIGHTNING MODEL\n",
    "    # ------------------------\n",
    "    model = GAN(hparams)\n",
    "\n",
    "    # ------------------------\n",
    "    # 2 INIT TRAINER\n",
    "    # ------------------------\n",
    "    trainer = pl.Trainer()\n",
    "\n",
    "    # ------------------------\n",
    "    # 3 START TRAINING\n",
    "    # ------------------------\n",
    "    trainer.fit(model)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
    "    parser.add_argument(\"--b1\", type=float, default=0.5,\n",
    "                        help=\"adam: decay of first order momentum of gradient\")\n",
    "    parser.add_argument(\"--b2\", type=float, default=0.999,\n",
    "                        help=\"adam: decay of first order momentum of gradient\")\n",
    "    parser.add_argument(\"--latent_dim\", type=int, default=100,\n",
    "                        help=\"dimensionality of the latent space\")\n",
    "\n",
    "    hparams = parser.parse_args()\n",
    "\n",
    "    main(hparams)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
