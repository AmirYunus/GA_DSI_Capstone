{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>TECHNICAL REPORT - FINANCIAL FORENSIC DATA ANALYSIS<span class=\"tocSkip\"></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Amir Yunus<br>\n",
    "GitHub: https://github.com/AmirYunus/GA_DSI_Capstone\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREFACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: Fraud and the Accounting System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Association of Certified Fraud Examiners estimates in its Global Study on Occupational Fraud and Abuse, 2018 [[1]](../documents/2018-report-to-the-nations.pdf) that organisations lose 5% of their annual revenues to fraud. In their report, the term **\"occupational fraud\"** refers to, \n",
    "\n",
    ">_\" . . . an attack against the organisation from within, by the very people who were entrusted to protect its assets and resources\"_.\n",
    "\n",
    "A similar study, conducted by the auditors of PwC [[2]](../documents/global-economic-crime-and-fraud-survey-2018.pdf), revealed that 30% of the study respondents experienced losses of between USD 100,000 and USD 5 million in the last 24 months (as of 2018). The study also showed that financial statement fraud caused by far the greatest median loss of the surveyed fraud schemes.\n",
    "\n",
    "At the same time organizations accelerate the digitisation and reconfiguration of business processes [[3]](../documents/accelerating_the_digitization_of_business_processes.pdf) affecting in particular Accounting Information Systems (AIS) or more general Enterprise Resource Planning (ERP) systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"height: auto\" src=\"../images/accounting.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 1:** Hierarchical view of an Accounting Information System (AIS) that records distinct layers of abstraction, namely (1) the business process information, (2) the accounting information as well as the (3) technical journal entry information in designated database tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steadily, these systems collect vast quantities of electronic evidence at an almost atomic level. This holds in particular for the journal entries of an organization recorded in its general ledger and sub-ledger accounts. SAP, one of the most prominent ERP software providers, estimates that approx. 76% of the world's transaction revenue touches one of their systems [5].\n",
    "\n",
    "The illustration in **Figure 1** depicts a hierarchical view of an Accounting Information System (AIS) recording process and journal entry information in designated database tables. In the context of fraud examinations, the data collected by such systems may contain valuable traces of a potential fraud scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of Financial Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When conducting a detailed examination of real-world journal entries, usually recorded in large-scaled AIS or ERP systems, two prevalent characteristics can be observed:\n",
    "\n",
    "> - specific transactions attributes exhibit **a high variety of distinct attribute values** e.g. customer information, posted sub-ledgers, amount information, and \n",
    "> - the transactions exhibit **strong dependencies between specific attribute values** e.g. between customer information and type of payment, posting type and general ledgers. \n",
    "\n",
    "Derived from this observation we distinguish two classes of anomalous journal entries, namely **\"global\"** and **\"local\" anomalies** as illustrated in **Figure 2** below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"height: auto\" src=\"../images/anomalies.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 2:** Illustrative example of global and local anomalies portrait in a feature space of the two transaction features \"Posting Amount\" (Feature 1) and \"Posting Positions\" (Feature 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Global Anomalies***, are financial transactions that exhibit **unusual or rare individual attribute values**. These anomalies usually relate to highly skewed attributes e.g. seldom posting users, rarely used ledgers, or unusual posting times. \n",
    "\n",
    "Traditionally \"red-flag\" tests, performed by auditors during annual audits, are designed to capture those types of anomalies. However, such tests might result in a high volume of false positive alerts due to e.g. regular reverse postings, provisions and year-end adjustments usually associated with a low fraud risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Local Anomalies***, are financial transactions that exhibit an **unusual or rare combination of attribute values** while the individual attribute values occur quite frequently e.g. unusual accounting records. \n",
    "\n",
    "This type of anomaly is significantly more difficult to detect since perpetrators intend to disguise their activities trying to imitate a regular behaviour. As a result, such anomalies usually pose a high fraud risk since they might correspond to e.g. misused user accounts, irregular combinations of general ledger accounts and posting keys that don't follow an usual activity pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this lab is to walk you through a deep learning based methodology that can be used to detect of global and local anomalies in financial datasets. The proposed method is based on the following assumptions: \n",
    "\n",
    ">1. the majority of financial transactions recorded within an organizations’ ERP-system relate to regular day-to-day business activities and perpetrators need to deviate from the ”regular” in order to conduct fraud,\n",
    ">2. such deviating behaviour will be recorded by a very limited number of financial transactions and their respective attribute values or combination of attribute values and we refer to such deviation as \"anomaly\".\n",
    "\n",
    "Concluding from these assumptions we can learn a model of regular journal entries with minimal ”harm” caused by the potential anomalous ones.\n",
    "\n",
    "In order to detect such anomalies, we will train deep autoencoder networks to learn a compressed but \"lossy\" model of regular transactions and their underlying posting pattern. Imposing a strong regularization onto the network hidden layers limits the networks' ability to memorize the characteristics of anomalous journal entries. Once the training process is completed, the network will be able to reconstruct regular journal entries, while failing to do so for the anomalous ones.\n",
    "\n",
    "After completing the lab you should be familiar with:\n",
    "\n",
    ">1. the basic concepts, intuitions and major building blocks of autoencoder neural networks,\n",
    ">2. the techniques of pre-processing financial data in order to learn a model of its characteristics,\n",
    ">3. the application of autoencoder neural networks to detect anomalies in large-scale financial data, and,\n",
    ">4. the interpretation of the detection results of the networks as well as its reconstruction loss. \n",
    "\n",
    "Please note, that this lab is neither a complete nor comprehensive forensic data analysis approach or fraud examination strategy. However, the methodology and code provided in this lab can be modified or adapted to detect anomalous records in a variety of financial datasets. Subsequently, the detected records might serve as a starting point for a more detailed and substantive examination by auditors or compliance personnel. \n",
    "\n",
    "For this lab, we assume that you are familiar with the general concepts of deep neural networks (DNN) and GPUs as well as PyTorch and Python. For more information on these concepts please check the relevant labs of NVIDIA's Deep Learning Institute (DLI). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about potential fraud scenarios of your organization:\n",
    "\n",
    ">1. What scenarios or fraudulent activities you could think of? [3 min]\n",
    ">2. What data sources might affect or record those potential fraudulent activities? [5 min]\n",
    ">3. What kind of data analytics techniques could be applied to detect those activities? [5 min]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#PREFACE\" data-toc-modified-id=\"PREFACE-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>PREFACE</a></span><ul class=\"toc-item\"><li><span><a href=\"#Background:-Fraud-and-the-Accounting-System\" data-toc-modified-id=\"Background:-Fraud-and-the-Accounting-System-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Background: Fraud and the Accounting System</a></span></li><li><span><a href=\"#Classification-of-Financial-Anomalies\" data-toc-modified-id=\"Classification-of-Financial-Anomalies-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Classification of Financial Anomalies</a></span></li><li><span><a href=\"#Executive-Summary\" data-toc-modified-id=\"Executive-Summary-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Executive Summary</a></span></li><li><span><a href=\"#Content\" data-toc-modified-id=\"Content-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Content</a></span></li><li><span><a href=\"#Data-Dictionary\" data-toc-modified-id=\"Data-Dictionary-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Data Dictionary</a></span></li><li><span><a href=\"#Libraries\" data-toc-modified-id=\"Libraries-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Libraries</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Basic-Utilities\" data-toc-modified-id=\"Basic-Utilities-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>Basic Utilities</a></span></li><li><span><a href=\"#Define-Networks\" data-toc-modified-id=\"Define-Networks-1.7.2\"><span class=\"toc-item-num\">1.7.2&nbsp;&nbsp;</span>Define Networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Encoder\" data-toc-modified-id=\"Encoder-1.7.2.1\"><span class=\"toc-item-num\">1.7.2.1&nbsp;&nbsp;</span>Encoder</a></span></li><li><span><a href=\"#Decoder\" data-toc-modified-id=\"Decoder-1.7.2.2\"><span class=\"toc-item-num\">1.7.2.2&nbsp;&nbsp;</span>Decoder</a></span></li><li><span><a href=\"#Discriminator\" data-toc-modified-id=\"Discriminator-1.7.2.3\"><span class=\"toc-item-num\">1.7.2.3&nbsp;&nbsp;</span>Discriminator</a></span></li><li><span><a href=\"#Autoencoder\" data-toc-modified-id=\"Autoencoder-1.7.2.4\"><span class=\"toc-item-num\">1.7.2.4&nbsp;&nbsp;</span>Autoencoder</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#ABOUT-THE-DATASET\" data-toc-modified-id=\"ABOUT-THE-DATASET-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>ABOUT THE DATASET</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importing-the-Dataset\" data-toc-modified-id=\"Importing-the-Dataset-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Importing the Dataset</a></span></li></ul></li><li><span><a href=\"#EXPLORATORY-DATA-ANALYSIS\" data-toc-modified-id=\"EXPLORATORY-DATA-ANALYSIS-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>EXPLORATORY DATA ANALYSIS</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Attributes\" data-toc-modified-id=\"Data-Attributes-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Data Attributes</a></span></li><li><span><a href=\"#Exploring-Dataset-Using-Benford's-Law\" data-toc-modified-id=\"Exploring-Dataset-Using-Benford's-Law-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Exploring Dataset Using Benford's Law</a></span></li><li><span><a href=\"#Categorical-Features\" data-toc-modified-id=\"Categorical-Features-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Categorical Features</a></span><ul class=\"toc-item\"><li><span><a href=\"#BELNR\" data-toc-modified-id=\"BELNR-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>BELNR</a></span></li><li><span><a href=\"#WAERS\" data-toc-modified-id=\"WAERS-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>WAERS</a></span></li><li><span><a href=\"#BUKRS\" data-toc-modified-id=\"BUKRS-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>BUKRS</a></span></li><li><span><a href=\"#KTOSL\" data-toc-modified-id=\"KTOSL-3.3.4\"><span class=\"toc-item-num\">3.3.4&nbsp;&nbsp;</span>KTOSL</a></span></li><li><span><a href=\"#PRCTR\" data-toc-modified-id=\"PRCTR-3.3.5\"><span class=\"toc-item-num\">3.3.5&nbsp;&nbsp;</span>PRCTR</a></span></li><li><span><a href=\"#BSCHL\" data-toc-modified-id=\"BSCHL-3.3.6\"><span class=\"toc-item-num\">3.3.6&nbsp;&nbsp;</span>BSCHL</a></span></li><li><span><a href=\"#HKONT\" data-toc-modified-id=\"HKONT-3.3.7\"><span class=\"toc-item-num\">3.3.7&nbsp;&nbsp;</span>HKONT</a></span></li></ul></li><li><span><a href=\"#Numerical-Features\" data-toc-modified-id=\"Numerical-Features-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Numerical Features</a></span><ul class=\"toc-item\"><li><span><a href=\"#DMBTR\" data-toc-modified-id=\"DMBTR-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>DMBTR</a></span></li><li><span><a href=\"#WRBTR\" data-toc-modified-id=\"WRBTR-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>WRBTR</a></span></li></ul></li></ul></li><li><span><a href=\"#FEATURE-ENGINEERING\" data-toc-modified-id=\"FEATURE-ENGINEERING-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>FEATURE ENGINEERING</a></span><ul class=\"toc-item\"><li><span><a href=\"#One-Hot-Encoding\" data-toc-modified-id=\"One-Hot-Encoding-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>One Hot Encoding</a></span></li><li><span><a href=\"#Log-Transform\" data-toc-modified-id=\"Log-Transform-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Log Transform</a></span></li><li><span><a href=\"#Merge-Features\" data-toc-modified-id=\"Merge-Features-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Merge Features</a></span></li></ul></li><li><span><a href=\"#DBSCAN\" data-toc-modified-id=\"DBSCAN-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>DBSCAN</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib\n",
    "from numpy.random import seed\n",
    "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display, Markdown\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(color_codes=True)\n",
    "%matplotlib inline\n",
    "# tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(10)\n",
    "tf.compat.v1.set_random_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import utilities\n",
    "# import os\n",
    "# import sys\n",
    "# import random\n",
    "# import io\n",
    "# import urllib\n",
    "# import warnings\n",
    "# import IPython\n",
    "# import csv\n",
    "\n",
    "# from IPython.display import display, Markdown\n",
    "# from datetime import datetime\n",
    "\n",
    "# # import data science libraries\n",
    "# import pandas as pd\n",
    "# import random as rd\n",
    "# import numpy as np\n",
    "\n",
    "# # import pytorch libraries\n",
    "# import torch\n",
    "# import torch.optim as optim\n",
    "# import pytorch_lightning as pl\n",
    "# from torch import nn\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.nn import functional as F\n",
    "\n",
    "# # import python plotting libraries\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# Prevent warnings from distracting the reader\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Colour scheme and style selected\n",
    "theme = ['#1F306E', '#553772', '#8F3B76', '#C7417B', '#F5487F']\n",
    "colors_palette = sns.palplot(sns.color_palette(theme))\n",
    "plt.style.use('seaborn')\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "sns.set_palette(colors_palette)\n",
    "\n",
    "# Forces Matplotlib to use high-quality images\n",
    "ip = get_ipython()\n",
    "ibe = ip.configurables[-1]\n",
    "ibe.figure_formats = {'pdf', 'png'}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../data'): os.makedirs('../data')  # create data directory\n",
    "if not os.path.exists('../models'): os.makedirs('../models')  # create trained models directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Define LOG\n",
    "##################################\n",
    "def log ():\n",
    "    now = str(datetime.now())\n",
    "    print(f'[LOG {now}]')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################\n",
    "# # Define Euclidean Distance Calculation\n",
    "# ##################################\n",
    "# def compute_euclid_distance(x, y):\n",
    "    \n",
    "#     # calculate euclidean distance \n",
    "#     euclidean_distance = np.sqrt(np.sum((x - y) ** 2, axis=1))\n",
    "    \n",
    "#     # return euclidean distance\n",
    "#     return euclidean_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Define Encoder Model - in/16/z\n",
    "##################################\n",
    "def encoder_model (X):\n",
    "    # input\n",
    "    model_input = Input(shape=(X.shape[1], X.shape[2]))\n",
    "    \n",
    "    # encoder\n",
    "    L01 = LeakyReLU(16, return_sequences=True, kernel_regularizer=regularizers.l2(0.00))(model_input)\n",
    "    model_output = LeakyReLU(4, return_sequences=False)(L01)\n",
    "    \n",
    "    # define model\n",
    "    model = Model(inputs=model_input, outputs=model_output)\n",
    "    \n",
    "    # return model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Define Decoder Model - z/16/out\n",
    "##################################\n",
    "def decoder_model (X):\n",
    "    # input\n",
    "    model_input = Input(shape=(X.shape[1]))\n",
    "    \n",
    "    # decoder\n",
    "    L01 = LeakyReLU(16, return_sequences=True)(model_input)\n",
    "    model_output = Dense(X.shape[2], activation = 'sigmoid', return_sequences=True)(L01)\n",
    "    \n",
    "    # define model\n",
    "    model = Model(inputs=model_input, outputs=model_output)\n",
    "    \n",
    "    # return model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Define Discriminator Model - z/16/z\n",
    "##################################\n",
    "def discriminator_model (X):\n",
    "    # input\n",
    "    model_input = Input(shape=(X.shape[1]))\n",
    "    \n",
    "    # decoder\n",
    "    L01 = LeakyReLU(16, return_sequences=True)(model_input)\n",
    "    model_output = Dense(4, activation = 'sigmoid', return_sequences=True)(L01)\n",
    "    \n",
    "    # define model\n",
    "    model = Model(inputs=model_input, outputs=model_output)\n",
    "    \n",
    "    # return model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Define Autoencoder Model\n",
    "##################################\n",
    "def autoencoder_model (X):\n",
    "    # input\n",
    "    model_input = Input(shape=(X.shape[1], X.shape[2]))\n",
    "    \n",
    "    # encoder\n",
    "    L01 = LSTM(512, activation = tf.nn.leaky_relu, return_sequences=True, kernel_regularizer=regularizers.l2(0.00))(model_input)\n",
    "    L02 = LSTM(256, activation = tf.nn.leaky_relu, return_sequences=True)(L01)\n",
    "    L03 = LSTM(128, activation = tf.nn.leaky_relu, return_sequences=True)(L02)\n",
    "    L04 = LSTM(64, activation = tf.nn.leaky_relu, return_sequences=True)(L03)\n",
    "    L05 = LSTM(32, activation = tf.nn.leaky_relu, return_sequences=True)(L04)\n",
    "    L06 = LSTM(16, activation = tf.nn.leaky_relu, return_sequences=True)(L05)\n",
    "    L07 = LSTM(4, activation = tf.nn.leaky_relu, return_sequences=True)(L06)\n",
    "    L08 = LSTM(2, activation = tf.nn.leaky_relu, return_sequences=False)(L07)\n",
    "    \n",
    "    # latent space\n",
    "    L09 = RepeatVector(X.shape[1])(L08)\n",
    "    \n",
    "    # decoder\n",
    "    L10 = LSTM(2, activation = tf.nn.leaky_relu, return_sequences=True)(L09)\n",
    "    L11 = LSTM(4, activation = tf.nn.leaky_relu, return_sequences=True)(L10)\n",
    "    L12 = LSTM(16, activation = tf.nn.leaky_relu, return_sequences=True)(L11)\n",
    "    L13 = LSTM(32, activation = tf.nn.leaky_relu, return_sequences=True)(L12)\n",
    "    L14 = LSTM(64, activation = tf.nn.leaky_relu, return_sequences=True)(L13)\n",
    "    L15 = LSTM(128, activation = tf.nn.leaky_relu, return_sequences=True)(L14)\n",
    "    L16 = LSTM(256, activation = tf.nn.leaky_relu, return_sequences=True)(L15)\n",
    "    L17 = LSTM(512, activation = tf.nn.leaky_relu, return_sequences=True)(L16)\n",
    "    \n",
    "    # output\n",
    "    model_output = TimeDistributed(Dense(X.shape[2]))(L17)\n",
    "    \n",
    "    # define model\n",
    "    model = Model(inputs=model_input, outputs=model_output)\n",
    "    \n",
    "    # return model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ##################################\n",
    "# # # Define Autoencoder Model\n",
    "# # ##################################\n",
    "# def autoencoder_model (X):\n",
    "#     # input\n",
    "#     model_input = Input(shape=(X.shape[1], X.shape[2]))\n",
    "    \n",
    "#     # encoder\n",
    "#     L01 = LSTM(512, activation = tf.nn.leaky_relu, return_sequences=True, kernel_regularizer=regularizers.l2(0.00))(model_input)\n",
    "#     L02 = LSTM(256, activation = tf.nn.leaky_relu, return_sequences=True)(L01)\n",
    "#     L03 = LSTM(128, activation = tf.nn.leaky_relu, return_sequences=True)(L02)\n",
    "#     L04 = LSTM(64, activation = tf.nn.leaky_relu, return_sequences=True)(L03)\n",
    "#     L05 = LSTM(32, activation = tf.nn.leaky_relu, return_sequences=True)(L04)\n",
    "#     L06 = LSTM(16, activation = tf.nn.leaky_relu, return_sequences=True)(L05)\n",
    "#     L07 = LSTM(4, activation = tf.nn.leaky_relu, return_sequences=True)(L06)\n",
    "#     L08 = LSTM(2, activation = tf.nn.leaky_relu, return_sequences=False)(L07)\n",
    "    \n",
    "#     # latent space\n",
    "#     L09 = RepeatVector(X.shape[1])(L08)\n",
    "    \n",
    "#     # decoder\n",
    "#     L10 = LSTM(2, activation = tf.nn.leaky_relu, return_sequences=True)(L09)\n",
    "#     L11 = LSTM(4, activation = tf.nn.leaky_relu, return_sequences=True)(L10)\n",
    "#     L12 = LSTM(16, activation = tf.nn.leaky_relu, return_sequences=True)(L11)\n",
    "#     L13 = LSTM(32, activation = tf.nn.leaky_relu, return_sequences=True)(L12)\n",
    "#     L14 = LSTM(64, activation = tf.nn.leaky_relu, return_sequences=True)(L13)\n",
    "#     L15 = LSTM(128, activation = tf.nn.leaky_relu, return_sequences=True)(L14)\n",
    "#     L16 = LSTM(256, activation = tf.nn.leaky_relu, return_sequences=True)(L15)\n",
    "#     L17 = LSTM(512, activation = tf.nn.leaky_relu, return_sequences=True)(L16)\n",
    "    \n",
    "#     # output\n",
    "#     model_output = TimeDistributed(Dense(X.shape[2]))(L17)\n",
    "    \n",
    "#     # define model\n",
    "#     model = Model(inputs=model_input, outputs=model_output)\n",
    "    \n",
    "#     # return model\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ##################################\n",
    "# # # Define Autoencoder Model - 16/4/z/4/16\n",
    "# # ##################################\n",
    "# def autoencoder_model (X):\n",
    "#     # input\n",
    "#     model_input = Input(shape=(X.shape[1], X.shape[2]))\n",
    "    \n",
    "#     # encoder\n",
    "#     L01 = Dense(16, activation = tf.nn.leaky_relu, return_sequences=True, kernel_regularizer=regularizers.l2(0.00))(model_input)\n",
    "#     L02 = Dense(4, activation = tf.nn.leaky_relu, return_sequences=False)(L01)\n",
    "    \n",
    "#     # latent space\n",
    "#     L03 = RepeatVector(X.shape[1])(L02)\n",
    "    \n",
    "#     # decoder\n",
    "#     L04 = Dense(4, activation = tf.nn.leaky_relu, return_sequences=True)(L03)\n",
    "#     L05 = Dense(16, activation = tf.nn.leaky_relu, return_sequences=True)(L04)\n",
    "    \n",
    "#     # output\n",
    "#     model_output = TimeDistributed(Dense(X.shape[2]))(L05)\n",
    "    \n",
    "#     # define model\n",
    "#     model = Model(inputs=model_input, outputs=model_output)\n",
    "    \n",
    "#     # return model\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define encoder class\n",
    "# class Encoder(nn.Module):\n",
    "\n",
    "#     # define class constructor\n",
    "#     def __init__(self, input_size, hidden_size):\n",
    "\n",
    "#         # call super class constructor\n",
    "#         super(Encoder, self).__init__()\n",
    "\n",
    "#         # specify first layer - in 618, out 256\n",
    "#         self.map_L1 = nn.Linear(input_size, hidden_size[0], bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.map_L1.weight) # init weights according to [9]\n",
    "#         nn.init.constant_(self.map_L1.bias, 0.0) # constant initialization of the bias\n",
    "#         self.map_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
    "\n",
    "#         # specify second layer - in 256, out 64\n",
    "#         self.map_L2 = nn.Linear(hidden_size[0], hidden_size[1], bias=True)\n",
    "#         nn.init.xavier_uniform_(self.map_L2.weight)\n",
    "#         nn.init.constant_(self.map_L2.bias, 0.0)\n",
    "#         self.map_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "#         # specify third layer - in 64, out 16\n",
    "#         self.map_L3 = nn.Linear(hidden_size[1], hidden_size[2], bias=True)\n",
    "#         nn.init.xavier_uniform_(self.map_L3.weight)\n",
    "#         nn.init.constant_(self.map_L3.bias, 0.0)\n",
    "#         self.map_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "#         # specify fourth layer - in 16, out 4\n",
    "#         self.map_L4 = nn.Linear(hidden_size[2], hidden_size[3], bias=True)\n",
    "#         nn.init.xavier_uniform_(self.map_L4.weight)\n",
    "#         nn.init.constant_(self.map_L4.bias, 0.0)\n",
    "#         self.map_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "#         # specify fifth layer - in 4, out 2\n",
    "#         self.map_L5 = nn.Linear(hidden_size[3], hidden_size[4], bias=True)\n",
    "#         nn.init.xavier_uniform_(self.map_L5.weight)\n",
    "#         nn.init.constant_(self.map_L5.bias, 0.0)\n",
    "#         self.map_R5 = torch.nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "        \n",
    "#     # define forward pass\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         # run forward pass through the network\n",
    "#         x = self.map_R1(self.map_L1(x))\n",
    "#         x = self.map_R2(self.map_L2(x))\n",
    "#         x = self.map_R3(self.map_L3(x))\n",
    "#         x = self.map_R4(self.map_L4(x))\n",
    "#         x = self.map_R5(self.map_L5(x))\n",
    "\n",
    "#         # return result\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################\n",
    "# # Define Encoder\n",
    "# ##################################\n",
    "# class encoder(nn.Module):\n",
    "\n",
    "#     # define class constructor\n",
    "#     def __init__(self,input_size):\n",
    "        \n",
    "#         # call super class constructor\n",
    "#         super(encoder, self).__init__()\n",
    "        \n",
    "#         # first layer - in input size, out 512\n",
    "#         self.l1 = nn.Linear(input_size, 512, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l1.weight) # init weights according to [9]\n",
    "#         nn.init.constant_(self.l1.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
    "\n",
    "#         # second layer - in 512, out 256\n",
    "#         self.l2 = nn.Linear(512, 256, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l2.weight) # init weights\n",
    "#         nn.init.constant_(self.l2.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r2 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    "\n",
    "#         # third layer - in 256, out 128\n",
    "#         self.l3 = nn.Linear(256, 128, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l3.weight) # init weights\n",
    "#         nn.init.constant_(self.l3.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r3 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    "        \n",
    "#         # fourth layer - in 128, out 64\n",
    "#         self.l4 = nn.Linear(128, 64, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l4.weight) # init weights\n",
    "#         nn.init.constant_(self.l4.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r4 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    "        \n",
    "#         # fifth layer - in 64, out 32\n",
    "#         self.l5 = nn.Linear(64, 32, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l5.weight) # init weights\n",
    "#         nn.init.constant_(self.l5.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r5 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    "\n",
    "#         # sixth layer - in 32, out 16\n",
    "#         self.l6 = nn.Linear(32, 16, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l6.weight) # init weights\n",
    "#         nn.init.constant_(self.l6.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r6 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    "\n",
    "#         # seventh layer - in 16, out 8\n",
    "#         self.l7 = nn.Linear(16, 8, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l7.weight) # init weights\n",
    "#         nn.init.constant_(self.l7.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r7 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    "        \n",
    "#         # eigth layer - in 8, out 4\n",
    "#         self.l8 = nn.Linear(8, 4, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l8.weight) # init weights\n",
    "#         nn.init.constant_(self.l8.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r8 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    " \n",
    "#         # ninth layer - in 4, out 2\n",
    "#         self.l9 = nn.Linear(4, 2, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l9.weight) # init weights\n",
    "#         nn.init.constant_(self.l9.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r9 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    "\n",
    "#     # define forward pass\n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         # run forward pass through the network\n",
    "#         x = self.r1(self.l1(x))\n",
    "#         x = self.r2(self.l2(x))\n",
    "#         x = self.r3(self.l3(x))\n",
    "#         x = self.r4(self.l4(x))\n",
    "#         x = self.r5(self.l5(x))\n",
    "#         x = self.r6(self.l6(x))\n",
    "#         x = self.r7(self.l7(x))\n",
    "#         x = self.r8(self.l8(x))\n",
    "#         x = self.r9(self.l9(x))\n",
    "        \n",
    "#         # return result\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ##################################\n",
    "# # # Define Decoder Model\n",
    "# # ##################################\n",
    "# def decoder_model (X):\n",
    "#     # input\n",
    "#     model_input = Input(shape = X.shape[1])\n",
    "    \n",
    "#     # decoder\n",
    "#     L01 = LSTM(2, activation = tf.nn.leaky_relu, return_sequences=True)(model_input)\n",
    "#     L02 = LSTM(4, activation = tf.nn.leaky_relu, return_sequences=True)(L01)\n",
    "#     L03 = LSTM(16, activation = tf.nn.leaky_relu, return_sequences=True)(L02)\n",
    "#     L04 = LSTM(32, activation = tf.nn.leaky_relu, return_sequences=True)(L03)\n",
    "#     L05 = LSTM(64, activation = tf.nn.leaky_relu, return_sequences=True)(L04)\n",
    "#     L06 = LSTM(128, activation = tf.nn.leaky_relu, return_sequences=True)(L05)\n",
    "#     L07 = LSTM(256, activation = tf.nn.leaky_relu, return_sequences=True)(L06)\n",
    "#     L08 = LSTM(512, activation = tf.nn.leaky_relu, return_sequences=True)(L07)\n",
    "    \n",
    "#     # output\n",
    "#     model_output = TimeDistributed(Dense(X.shape[2]))(L08)\n",
    "    \n",
    "#     # define model\n",
    "#     model = Model(inputs=model_input, outputs=model_output)\n",
    "    \n",
    "#     # return model\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define decoder class\n",
    "# class Decoder(nn.Module):\n",
    "\n",
    "#     # define class constructor\n",
    "#     def __init__(self, output_size, hidden_size):\n",
    "\n",
    "#         # call super class constructor\n",
    "#         super(Decoder, self).__init__()\n",
    "\n",
    "#         # specify first layer - in 2, out 4\n",
    "#         self.map_L1 = nn.Linear(hidden_size[0], hidden_size[1], bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.map_L1.weight) # init weights according to [9]\n",
    "#         nn.init.constant_(self.map_L1.bias, 0.0) # constant initialization of the bias\n",
    "#         self.map_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
    "\n",
    "#         # specify second layer - in 4, out 16\n",
    "#         self.map_L2 = nn.Linear(hidden_size[1], hidden_size[2], bias=True)\n",
    "#         nn.init.xavier_uniform_(self.map_L2.weight)\n",
    "#         nn.init.constant_(self.map_L2.bias, 0.0)\n",
    "#         self.map_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "#         # specify third layer - in 16, out 64\n",
    "#         self.map_L3 = nn.Linear(hidden_size[2], hidden_size[3], bias=True)\n",
    "#         nn.init.xavier_uniform_(self.map_L3.weight)\n",
    "#         nn.init.constant_(self.map_L3.bias, 0.0)\n",
    "#         self.map_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "#         # specify fourth layer - in 64, out 256\n",
    "#         self.map_L4 = nn.Linear(hidden_size[3], hidden_size[4], bias=True)\n",
    "#         nn.init.xavier_uniform_(self.map_L4.weight)\n",
    "#         nn.init.constant_(self.map_L4.bias, 0.0)\n",
    "#         self.map_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "        \n",
    "#         # specify fifth layer - in 256, out 618\n",
    "#         self.map_L5 = nn.Linear(hidden_size[4], output_size, bias=True)\n",
    "#         nn.init.xavier_uniform_(self.map_L5.weight)\n",
    "#         nn.init.constant_(self.map_L5.bias, 0.0)\n",
    "#         self.map_S5 = torch.nn.Sigmoid()\n",
    "\n",
    "#     # define forward pass\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         # run forward pass through the network\n",
    "#         x = self.map_R1(self.map_L1(x))\n",
    "#         x = self.map_R2(self.map_L2(x))\n",
    "#         x = self.map_R3(self.map_L3(x))\n",
    "#         x = self.map_R4(self.map_L4(x))\n",
    "#         x = self.map_S5(self.map_L5(x))\n",
    "\n",
    "#         # return result\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################\n",
    "# # Define Decoder\n",
    "# ##################################\n",
    "# class decoder(nn.Module):\n",
    "\n",
    "#     # define class constructor\n",
    "#     def __init__(self,output_size):\n",
    "        \n",
    "#         # call super class constructor\n",
    "#         super(decoder, self).__init__()\n",
    "        \n",
    "#         # first layer - in 2, out 4\n",
    "#         self.l1 = nn.Linear(2, 4, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l1.weight) # init weights according to [9]\n",
    "#         nn.init.constant_(self.l1.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
    "\n",
    "#         # second layer - in 4, out 8\n",
    "#         self.l2 = nn.Linear(4, 8, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l2.weight) # init weights\n",
    "#         nn.init.constant_(self.l2.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r2 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    "\n",
    "#         # third layer - in 8, out 16\n",
    "#         self.l3 = nn.Linear(8, 16, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l3.weight) # init weights\n",
    "#         nn.init.constant_(self.l3.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r3 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    "        \n",
    "#         # fourth layer - in 16, out 32\n",
    "#         self.l4 = nn.Linear(16, 32, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l4.weight) # init weights\n",
    "#         nn.init.constant_(self.l4.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r4 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    "        \n",
    "#         # fifth layer - in 32, out 64\n",
    "#         self.l5 = nn.Linear(32, 64, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l5.weight) # init weights\n",
    "#         nn.init.constant_(self.l5.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r5 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    "\n",
    "#         # sixth layer - in 64, out 128\n",
    "#         self.l6 = nn.Linear(64, 128, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l6.weight) # init weights\n",
    "#         nn.init.constant_(self.l6.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r6 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    "\n",
    "#         # seventh layer - in 128, out 256\n",
    "#         self.l7 = nn.Linear(128, 256, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l7.weight) # init weights\n",
    "#         nn.init.constant_(self.l7.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r7 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    "        \n",
    "#         # eigth layer - in 256, out 512\n",
    "#         self.l8 = nn.Linear(256, 512, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l8.weight) # init weights\n",
    "#         nn.init.constant_(self.l8.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r8 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    " \n",
    "#         # ninth layer - in 512, out output size\n",
    "#         self.l9 = nn.Linear(512, output_size, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l9.weight) # init weights\n",
    "#         nn.init.constant_(self.l9.bias, 0.0) # constant initialization of the bias\n",
    "#         self.s9 = nn.Sigmoid() # sigmoid transformation\n",
    "\n",
    "#     # define forward pass\n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         # run forward pass through the network\n",
    "#         x = self.r1(self.l1(x))\n",
    "#         x = self.r2(self.l2(x))\n",
    "#         x = self.r3(self.l3(x))\n",
    "#         x = self.r4(self.l4(x))\n",
    "#         x = self.r5(self.l5(x))\n",
    "#         x = self.r6(self.l6(x))\n",
    "#         x = self.r7(self.l7(x))\n",
    "#         x = self.r8(self.l8(x))\n",
    "#         x = self.s9(self.l9(x))\n",
    "        \n",
    "#         # return result\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define discriminator class\n",
    "# class Discriminator(nn.Module):\n",
    "\n",
    "#     # define class constructor\n",
    "#     def __init__(self, input_size, hidden_size, output_size):\n",
    "\n",
    "#         # call super class constructor\n",
    "#         super(Discriminator, self).__init__()\n",
    "\n",
    "#         # specify first layer - in 2, out 256\n",
    "#         self.map_L1 = nn.Linear(input_size, hidden_size[0], bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.map_L1.weight) # init weights according to [9]\n",
    "#         nn.init.constant_(self.map_L1.bias, 0.0) # constant initialization of the bias\n",
    "#         self.map_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
    "\n",
    "#         # specify second layer - in 256, out 16\n",
    "#         self.map_L2 = nn.Linear(hidden_size[0], hidden_size[1], bias=True)\n",
    "#         nn.init.xavier_uniform_(self.map_L2.weight)\n",
    "#         nn.init.constant_(self.map_L2.bias, 0.0)\n",
    "#         self.map_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "#         # specify third layer - in 16, out 4\n",
    "#         self.map_L3 = nn.Linear(hidden_size[1], hidden_size[2], bias=True)\n",
    "#         nn.init.xavier_uniform_(self.map_L3.weight)\n",
    "#         nn.init.constant_(self.map_L3.bias, 0.0)\n",
    "#         self.map_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "        \n",
    "#         # specify fourth layer - in 4, out 2\n",
    "#         self.map_L4 = nn.Linear(hidden_size[2], output_size, bias=True)\n",
    "#         nn.init.xavier_uniform_(self.map_L4.weight)\n",
    "#         nn.init.constant_(self.map_L4.bias, 0.0)\n",
    "#         self.map_S4 = torch.nn.Sigmoid()\n",
    "\n",
    "#     # define forward pass\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         # run forward pass through the network\n",
    "#         x = self.map_R1(self.map_L1(x))\n",
    "#         x = self.map_R2(self.map_L2(x))\n",
    "#         x = self.map_R3(self.map_L3(x))\n",
    "#         x = self.map_S4(self.map_L4(x))\n",
    "\n",
    "#         # return result\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################\n",
    "# # Define Discriminator\n",
    "# ##################################\n",
    "# class discriminator(nn.Module):\n",
    "\n",
    "#     # define class constructor\n",
    "#     def __init__(self):\n",
    "        \n",
    "#         # call super class constructor\n",
    "#         super(discriminator, self).__init__()\n",
    "        \n",
    "#         # first layer - in 2, out 512\n",
    "#         self.l1 = nn.Linear(2, 512, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l1.weight) # init weights according to [9]\n",
    "#         nn.init.constant_(self.l1.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
    "\n",
    "#         # second layer - in 512, out 256\n",
    "#         self.l2 = nn.Linear(512, 256, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l2.weight) # init weights\n",
    "#         nn.init.constant_(self.l2.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r2 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    "\n",
    "#         # third layer - in 256, out 128\n",
    "#         self.l3 = nn.Linear(256, 128, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l3.weight) # init weights\n",
    "#         nn.init.constant_(self.l3.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r3 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    "        \n",
    "#         # fourth layer - in 128, out 64\n",
    "#         self.l4 = nn.Linear(128, 64, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l4.weight) # init weights\n",
    "#         nn.init.constant_(self.l4.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r4 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    "        \n",
    "#         # fifth layer - in 64, out 32\n",
    "#         self.l5 = nn.Linear(64, 32, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l5.weight) # init weights\n",
    "#         nn.init.constant_(self.l5.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r5 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    "\n",
    "#         # sixth layer - in 32, out 16\n",
    "#         self.l6 = nn.Linear(32, 16, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l6.weight) # init weights\n",
    "#         nn.init.constant_(self.l6.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r6 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    "\n",
    "#         # seventh layer - in 16, out 8\n",
    "#         self.l7 = nn.Linear(16, 8, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l7.weight) # init weights\n",
    "#         nn.init.constant_(self.l7.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r7 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    "        \n",
    "#         # eigth layer - in 8, out 4\n",
    "#         self.l8 = nn.Linear(8, 4, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l8.weight) # init weights\n",
    "#         nn.init.constant_(self.l8.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r8 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    " \n",
    "#         # ninth layer - in 4, out 2\n",
    "#         self.l9 = nn.Linear(4, 2, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l9.weight) # init weights\n",
    "#         nn.init.constant_(self.l9.bias, 0.0) # constant initialization of the bias\n",
    "#         self.r9 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    "\n",
    "#         # tenth layer - in 2, out 1\n",
    "#         self.l0 = nn.Linear(2, 1, bias=True) # init linearity\n",
    "#         nn.init.xavier_uniform_(self.l0.weight) # init weights\n",
    "#         nn.init.constant_(self.l0.bias, 0.0) # constant initialization of the bias\n",
    "#         self.s0 = nn.Sigmoid() # sigmoid transformation\n",
    "\n",
    "#     # define forward pass\n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         # run forward pass through the network\n",
    "#         x = self.r1(self.l1(x))\n",
    "#         x = self.r2(self.l2(x))\n",
    "#         x = self.r3(self.l3(x))\n",
    "#         x = self.r4(self.l4(x))\n",
    "#         x = self.r5(self.l5(x))\n",
    "#         x = self.r6(self.l6(x))\n",
    "#         x = self.r7(self.l7(x))\n",
    "#         x = self.r8(self.l8(x))\n",
    "#         x = self.r9(self.l9(x))\n",
    "#         x = self.s0(self.l0(x))\n",
    "        \n",
    "#         # return result\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABOUT THE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will conduct a descriptive analysis of the labs financial dataset. Furthermore, we will apply some necessary pre-processing steps to train a deep neural network. The lab is based on a derivation of the **\"Synthetic Financial Dataset For Fraud Detection\"** by Lopez-Rojas [6] available via the Kaggle predictive modelling and analytics competitions platform that can be obtained using the following link: https://www.kaggle.com/ntnu-testimon/paysim1.\n",
    "\n",
    "Let's start loading the dataset and investigate its structure and attributes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "\n",
    "# load the dataset into the notebook kernel\n",
    "ori_dataset = pd.read_csv('../data/fraud_dataset_v2.csv')\n",
    "# inspect the datasets dimensionalities\n",
    "print(F'Transactional dataset of {ori_dataset.shape[0]} rows and {ori_dataset.shape[1]} columns loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We augmented the dataset and renamed the attributes to appear more similar to a real-world dataset that one usually observes in SAP-ERP systems as part of SAP's Finance and Cost controlling (FICO) module. \n",
    "\n",
    "The dataset contains a subset of in total 7 categorical and 2 numerical attributes available in the FICO BKPF (containing the posted journal entry headers) and BSEG (containing the posted journal entry segments) tables. Please, find below a list of the individual attributes as well as a brief description of their respective semantics:\n",
    "\n",
    ">- `BELNR`: the accounting document number,\n",
    ">- `BUKRS`: the company code,\n",
    ">- `BSCHL`: the posting key,\n",
    ">- `HKONT`: the posted general ledger account,\n",
    ">- `PRCTR`: the posted profit center,\n",
    ">- `WAERS`: the currency key,\n",
    ">- `KTOSL`: the general ledger account key,\n",
    ">- `DMBTR`: the amount in local currency,\n",
    ">- `WRBTR`: the amount in document currency.\n",
    "\n",
    "Let's also have a closer look into the top 10 rows of the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect top rows of dataset\n",
    "log()\n",
    "ori_dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also have noticed the attribute `label` in the data. We will use this field throughout the lab to evaluate the quality of our trained models. The field describes the true nature of each individual transaction of either being a **regular** transaction (denoted by `regular`) or an **anomaly** (denoted by `global` and `local`). Let's have closer look into the distribution of the regular vs. anomalous transactions in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# Display object type and values for each feature\n",
    "display(Markdown(f'<b>{ori_dataset.columns[9]}</b> - {ori_dataset.label.dtype}'))\n",
    "display(Markdown(f'Values:'))\n",
    "print(f'{ori_dataset.label.value_counts(normalize=True)}\\n')\n",
    "print()\n",
    "n_nan = ori_dataset.label.isnull().sum()\n",
    "if n_nan > 0:\n",
    "    print(f'{ori_dataset.label} has {n_nan} NaNs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# plot distribution of feature\n",
    "plt.figure(figsize=(16,9))\n",
    "sns.countplot(ori_dataset.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# number of anomalies vs. regular transactions\n",
    "ori_dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# number of anomalies vs. regular transactions\n",
    "ori_dataset.label.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the statistic reveals that, similar to real world scenarios, we are facing a highly \"unbalanced\" dataset. Overall, the dataset contains only a small fraction of **100 (0.018%)** anomalous transactions. While the 100 anomalous entries encompass **70 (0.013%)** \"global\" anomalies and **30 (0.005%)** \"local\" anomalies as introduced in section 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# remove the \"ground-truth\" label information for the following steps of the lab\n",
    "label = ori_dataset.pop('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Dataset Using Benford's Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benford's law is an observation about the frequency distribution of leading digits in many real-life sets of numerical data. The law states that in many naturally occurring collections of numbers, the leading significant digit is likely to be small. For example, in sets that obey the law, the number 1 appears as the most significant digit about 30% of the time, while 9 appears as the most significant digit less than 5% of the time. If the digits were distributed uniformly, they would each occur about 11.1% of the time. Benford's law also makes predictions about the distribution of second digits, third digits, digit combinations, and so on. - From [Wikipedia](https://en.wikipedia.org/wiki/Benford%27s_law)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# make a copy of original dataset\n",
    "ori_dataset_bf = ori_dataset.copy()\n",
    "\n",
    "# map out the first digit and display top 5 rows\n",
    "ori_dataset_bf['FIRST_DIGIT'] = ori_dataset_bf.DMBTR.map(lambda a: str(a)[0]).astype(int)\n",
    "ori_dataset_bf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# display the actual percentage distribution of dataset\n",
    "actuals = ori_dataset_bf.FIRST_DIGIT.value_counts(normalize=True).sort_index()\n",
    "actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log()\n",
    "# calculate the expected distribution based on Benford's Law\n",
    "digits = list(range(1,10))\n",
    "benford = [np.log10(1 + 1/d) for d in digits]\n",
    "plt.figure(figsize = (16,9))\n",
    "\n",
    "# plot graph to visualise distribution\n",
    "plt.bar(digits, benford, label='Exptected')\n",
    "plt.plot(actuals, color='r', label='Actual')\n",
    "plt.xticks(digits)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the initial data assessment above we can observe that the majority of attributes recorded in AIS- and ERP-systems correspond to categorical (discrete) attribute values, e.g. the posting date, the general-ledger account, the posting type, the currency. Let's have a more detailed look into the distribution of two dataset attributes, namely (1) the posting key `BSCHL` as well as (2) the general ledger account `HKONT`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BELNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# Display object type and values for each feature\n",
    "display(Markdown(f'<b>{ori_dataset.columns[0]}</b> - {ori_dataset.BELNR.dtype}'))\n",
    "display(Markdown(f'Values:'))\n",
    "print(f'{ori_dataset.BELNR.value_counts(normalize=True)}\\n')\n",
    "print()\n",
    "n_nan = ori_dataset.BELNR.isnull().sum()\n",
    "if n_nan > 0:\n",
    "    print(f'{ori_dataset.BELNR} has {n_nan} NaNs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log()\n",
    "# plot distribution of feature\n",
    "plt.figure(figsize=(16,9))\n",
    "sns.distplot(ori_dataset.BELNR)\n",
    "plt.title('Distribution of BELNR observations', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WAERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# Display object type and values for each feature\n",
    "display(Markdown(f'<b>{ori_dataset.columns[1]}</b> - {ori_dataset.WAERS.dtype}'))\n",
    "display(Markdown(f'Values:'))\n",
    "print(f'{ori_dataset.WAERS.value_counts()}\\n')\n",
    "print()\n",
    "n_nan = ori_dataset.WAERS.isnull().sum()\n",
    "if n_nan > 0:\n",
    "    print(f'{ori_dataset.WAERS} has {n_nan} NaNs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# prepare to plot posting key and general ledger account side by side\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figwidth(16)\n",
    "fig.set_figheight(9)\n",
    "\n",
    "# plot the distribution of the posting key attribute\n",
    "g = sns.countplot(x=ori_dataset.loc[label=='regular', 'WAERS'])\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=0)\n",
    "g.set_title('Distribution of WAERS observations', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUKRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# Display object type and values for each feature\n",
    "display(Markdown(f'<b>{ori_dataset.columns[2]}</b> - {ori_dataset.BUKRS.dtype}'))\n",
    "display(Markdown(f'Values:'))\n",
    "print(f'{ori_dataset.BUKRS.value_counts(normalize=True)}\\n')\n",
    "print()\n",
    "n_nan = ori_dataset.BUKRS.isnull().sum()\n",
    "if n_nan > 0:\n",
    "    print(f'{ori_dataset.BUKRS} has {n_nan} NaNs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# prepare to plot posting key and general ledger account side by side\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figwidth(16)\n",
    "fig.set_figheight(20)\n",
    "\n",
    "# plot the distribution of the posting key attribute\n",
    "g = sns.countplot(y=ori_dataset.loc[label=='regular', 'BUKRS'])\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=0)\n",
    "g.set_title('Distribution of BUKRS observations', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KTOSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# Display object type and values for each feature\n",
    "display(Markdown(f'<b>{ori_dataset.columns[3]}</b> - {ori_dataset.KTOSL.dtype}'))\n",
    "display(Markdown(f'Values:'))\n",
    "print(f'{ori_dataset.KTOSL.value_counts(normalize=True)}\\n')\n",
    "print()\n",
    "n_nan = ori_dataset.KTOSL.isnull().sum()\n",
    "if n_nan > 0:\n",
    "    print(f'{ori_dataset.KTOSL} has {n_nan} NaNs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# prepare to plot posting key and general ledger account side by side\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figwidth(16)\n",
    "fig.set_figheight(9)\n",
    "\n",
    "# plot the distribution of the posting key attribute\n",
    "g = sns.countplot(x=ori_dataset.loc[label=='regular', 'KTOSL'])\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=0)\n",
    "g.set_title('Distribution of KTOSL observations', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRCTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# Display object type and values for each feature\n",
    "display(Markdown(f'<b>{ori_dataset.columns[4]}</b> - {ori_dataset.PRCTR.dtype}'))\n",
    "display(Markdown(f'Values:'))\n",
    "print(f'{ori_dataset.PRCTR.value_counts(normalize=True)}\\n')\n",
    "print()\n",
    "n_nan = ori_dataset.PRCTR.isnull().sum()\n",
    "if n_nan > 0:\n",
    "    print(f'{ori_dataset.PRCTR} has {n_nan} NaNs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# prepare to plot posting key and general ledger account side by side\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figwidth(16)\n",
    "fig.set_figheight(20)\n",
    "\n",
    "# plot the distribution of the posting key attribute\n",
    "g = sns.countplot(y=ori_dataset.loc[label=='regular', 'PRCTR'])\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=0)\n",
    "g.set_title('Distribution of PRCTR observations', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BSCHL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# Display object type and values for each feature\n",
    "display(Markdown(f'<b>{ori_dataset.columns[5]}</b> - {ori_dataset.BSCHL.dtype}'))\n",
    "display(Markdown(f'Values:'))\n",
    "print(f'{ori_dataset.BSCHL.value_counts(normalize=True)}\\n')\n",
    "print()\n",
    "n_nan = ori_dataset.BSCHL.isnull().sum()\n",
    "if n_nan > 0:\n",
    "    print(f'{ori_dataset.BSCHL} has {n_nan} NaNs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# prepare to plot posting key and general ledger account side by side\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figwidth(16)\n",
    "fig.set_figheight(9)\n",
    "\n",
    "# plot the distribution of the posting key attribute\n",
    "g = sns.countplot(x=ori_dataset.loc[label=='regular', 'BSCHL'])\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=0)\n",
    "g.set_title('Distribution of BSCHL observations', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HKONT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# Display object type and values for each feature\n",
    "display(Markdown(f'<b>{ori_dataset.columns[6]}</b> - {ori_dataset.HKONT.dtype}'))\n",
    "display(Markdown(f'Values:'))\n",
    "print(f'{ori_dataset.HKONT.value_counts(normalize=True)}\\n')\n",
    "print()\n",
    "n_nan = ori_dataset.HKONT.isnull().sum()\n",
    "if n_nan > 0:\n",
    "    print(f'{ori_dataset.HKONT} has {n_nan} NaNs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# prepare to plot posting key and general ledger account side by side\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figwidth(16)\n",
    "fig.set_figheight(9)\n",
    "\n",
    "# plot the distribution of the posting key attribute\n",
    "g = sns.countplot(x=ori_dataset.loc[label=='regular', 'HKONT'])\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=0)\n",
    "g.set_title('Distribution of HKONT observations', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now inspect the distributions of the two numerical attributes contained in the transactional dataset namely, the (1) local currency amount `DMBTR` and the (2) document currency amount `WRBTR`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DMBTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# Display object type and values for each feature\n",
    "display(Markdown(f'<b>{ori_dataset.columns[7]}</b> - {ori_dataset.DMBTR.dtype}'))\n",
    "display(Markdown(f'Values:'))\n",
    "print(f'{ori_dataset.DMBTR.value_counts(normalize=True)}\\n')\n",
    "print()\n",
    "n_nan = ori_dataset.DMBTR.isnull().sum()\n",
    "if n_nan > 0:\n",
    "    print(f'{ori_dataset.DMBTR} has {n_nan} NaNs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# plot distribution of feature\n",
    "plt.figure(figsize=(16,9))\n",
    "sns.distplot(ori_dataset.DMBTR)\n",
    "plt.title('Distribution of DMBTR observations', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WRBTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# Display object type and values for each feature\n",
    "display(Markdown(f'<b>{ori_dataset.columns[8]}</b> - {ori_dataset.WRBTR.dtype}'))\n",
    "display(Markdown(f'Values:'))\n",
    "print(f'{ori_dataset.WRBTR.value_counts(normalize=True)}\\n')\n",
    "print()\n",
    "n_nan = ori_dataset.WRBTR.isnull().sum()\n",
    "if n_nan > 0:\n",
    "    print(f'{ori_dataset.WRBTR} has {n_nan} NaNs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# plot distribution of feature\n",
    "plt.figure(figsize=(16,9))\n",
    "sns.distplot(ori_dataset.WRBTR)\n",
    "plt.title('Distribution of WRBTR observations', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it can be observed, that for both attributes the distributions of amount values are heavy tailed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, neural networks are in general not designed to be trained directly on categorical data and require the attributes to be trained on to be numeric. One simple way to meet this requirement is by applying a technique referred to as **\"one-hot\" encoding**. Using this encoding technique, we will derive a numerical representation of each of the categorical attribute values. One-hot encoding creates new binary columns for each categorical attribute value present in the original data. \n",
    "\n",
    "Let's work through a brief example: The **categorical attribute “Receiver”** below contains the names \"John\", \"Timur\" and \"Marco\". We \"one-hot\" encode the names by creating a separate binary column for each possible name value observable in the \"Receiver\" column. Now, we encode for each transaction that contains the value \"John\" in the \"Receiver\" column this observation with 1.0 in the newly created \"John\" column and 0.0 in all other created name columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 430px; height: auto\" src=\"../images/encoding.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this technique will \"one-hot\" encode the 6 categorical attributes in the original transactional dataset. This can be achieved using the `get_dummies()` function available in the Pandas data science library:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select categorical attributes to be \"one-hot\" encoded\n",
    "log()\n",
    "\n",
    "categorical_attr_names = ['WAERS', 'BUKRS', 'KTOSL', 'PRCTR', 'BSCHL', 'HKONT']\n",
    "\n",
    "# encode categorical attributes into a binary one-hot encoded representation \n",
    "ori_dataset_categ_transformed = pd.get_dummies(ori_dataset[categorical_attr_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's inspect the encoding of 10 sample transactions to see if we have been successfull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect encoded sample transactions\n",
    "log()\n",
    "ori_dataset_categ_transformed.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the numeric features are heavily tailed. In order to approach faster a potential global minimum scaling and normalization of numerical input values is good a practice. Therefore, we first log-scale both variables and second min-max normalize the scaled amounts to the interval [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select \"DMBTR\" vs. \"WRBTR\" attribute\n",
    "log()\n",
    "\n",
    "numeric_attr_names = ['DMBTR', 'WRBTR']\n",
    "\n",
    "# add a small epsilon to eliminate zero values from data for log scaling\n",
    "numeric_attr = ori_dataset[numeric_attr_names] + 1e-7\n",
    "numeric_attr = numeric_attr.apply(np.log)\n",
    "\n",
    "# normalize all numeric attributes to the range [0,1]\n",
    "ori_dataset_numeric_attr = (numeric_attr - numeric_attr.min()) / (numeric_attr.max() - numeric_attr.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize the log-scaled and min-max normalized distributions of both attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the plots\n",
    "log()\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_figwidth(16)\n",
    "fig.set_figheight(9)\n",
    "\n",
    "# plot distribution of the local amount attribute\n",
    "g = sns.distplot(ori_dataset_numeric_attr['DMBTR'].tolist(), ax=ax[0])\n",
    "g.set_title('Distribution of scaled DMBTR amount values')\n",
    "\n",
    "# set axis-labels \n",
    "ax[0].set_xlabel('DMBTR')\n",
    "ax[0].set_ylabel('density')\n",
    "\n",
    "# plot distribution of the local amount attribute\n",
    "g = sns.distplot(ori_dataset_numeric_attr['WRBTR'].tolist(), ax=ax[1])\n",
    "g.set_title('Distribution of scaled WRBTR amount values')\n",
    "\n",
    "# set axis-labels\n",
    "ax[1].set_xlabel('WRBTR')\n",
    "ax[1].set_ylabel('density');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# plot distribution of feature\n",
    "plt.figure(figsize=(16,9))\n",
    "sns.distplot(ori_dataset_numeric_attr.DMBTR)\n",
    "plt.title('Distribution of scaled DMBTR observations', fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# plot distribution of feature\n",
    "plt.figure(figsize=(16,9))\n",
    "sns.distplot(ori_dataset_numeric_attr.WRBTR)\n",
    "plt.title('Distribution of scaled WRBTR observations', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's now visually investigate the scaled distributions of both attributes in terms of the distinct anomaly classes contained in the population of journal entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append 'label' attribute \n",
    "log()\n",
    "\n",
    "numeric_attr_vis = ori_dataset_numeric_attr.copy()\n",
    "numeric_attr_vis['label'] = label\n",
    "\n",
    "# plot the log-scaled and min-max normalized numeric attributes\n",
    "g = sns.pairplot(data=numeric_attr_vis, vars=numeric_attr_names, hue='label', diag_kind='kde', palette={'regular': 'C0', 'local': 'C3', 'global': 'C1'}, markers=['o', 'x', 'x'])\n",
    "\n",
    "# set figure title\n",
    "g.fig.suptitle('Distribution of DMBTR vs. WRBTR amount values', y=1.02, fontsize = 20)\n",
    "\n",
    "# set figure size\n",
    "g.fig.set_size_inches(16, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, as anticipated the numeric attribute values of the \"global\" anomalies (orange) fall outside the range of the regular amount distributions due to their unusual high amount values. In contrast, the numeric attribute values of the \"local\" anomalies (red) are much more commingled within the regular transaction amounts.\n",
    "As DMBTR attribute contains a number of extreme values we might want to visulalize its distribution by omitting those set of extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we merge both pre-processed numerical and categorical attributes into a single dataset that we will use for training our deep autoencoder neural network (explained an implemented in the following section 4.)\n",
    "\n",
    "Now, let's again have a look at the dimensionality of the dataset after we applied the distinct pre-processing steps to the attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log ()\n",
    "# merge categorical and numeric subsets\n",
    "ori_subset_transformed = pd.concat([ori_dataset_categ_transformed, ori_dataset_numeric_attr], axis = 1)\n",
    "\n",
    "# inspect final dimensions of pre-processed transactional data\n",
    "ori_subset_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the pre-processing steps above you may have noticed, that we didn't encode the attributes `WAERS` and `BUKRS` yet. This we left as an exercise for you:\n",
    "\n",
    ">1. Plot and inspect the distribution of the values of both attributes `WAERS` and `BUKRS`. [3 min]\n",
    ">2. Encode both variables using the `get_dummies()` method provided by the Pandas library. [5 min]\n",
    ">3. Merge your encoding results with the Pandas `ori_subset_transformed` data frame. [5 min]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, upon completion of all the pre-processing steps (incl. the exercises) we should end up with an encoded dataset consisting of a total number of 533,009 records (rows) and **618 encoded attributes** (columns). Let's keep the number number of columns in mind since it will define the dimensionality of the input- and output-layer of our deep autoencoder network which we will now implement in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_int = label.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(label_int)):\n",
    "    if label_int[i] == \"regular\":\n",
    "        label_int[i] = 0\n",
    "    elif label_int[i] == \"global\":\n",
    "        label_int[i] = 1\n",
    "    elif label_int[i] == \"local\":\n",
    "        label_int[i] = 0\n",
    "    else:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, df_y_train, df_y_test = \\\n",
    "train_test_split(\n",
    "    ori_subset_transformed, \n",
    "    label_int, \n",
    "    random_state=42, \n",
    "    stratify=label_int\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = \\\n",
    "train_test_split(\n",
    "    df_train, \n",
    "    df_y_train, \n",
    "    random_state=42, \n",
    "    stratify=df_y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = MinMaxScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_val = ss.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=200)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_val = pca.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "print(f'Training data shape {X_train.shape}')\n",
    "\n",
    "# X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "print(f'Testing data shape {X_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(\n",
    "    eps = .2,\n",
    "    metric='euclidean', \n",
    "    min_samples = 5,\n",
    "    n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dbscan.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = confusion_matrix(y_val.astype(int), pred)\n",
    "print(f'TP - True Negative {cmat[0,0]}')\n",
    "print(f'FP - False Positive {cmat[0,1]}')\n",
    "print(f'FN - False Negative {cmat[1,0]}')\n",
    "print(f'TP - True Positive {cmat[1,1]}')\n",
    "print(f'Accuracy Rate: {np.divide(np.sum([cmat[0,0],cmat[1,1]]),np.sum(cmat))}')\n",
    "print(f'Misclassification Rate: {np.divide(np.sum([cmat[0,1],cmat[1,0]]),np.sum(cmat))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val.astype(int), pred, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_int = label.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(label_int)):\n",
    "    if label_int[i] == \"regular\":\n",
    "        label_int[i] = 0\n",
    "    elif label_int[i] == \"global\":\n",
    "        label_int[i] = 0\n",
    "    elif label_int[i] == \"local\":\n",
    "        label_int[i] = 1\n",
    "    else:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, df_y_train, df_y_test = \\\n",
    "train_test_split(\n",
    "    ori_subset_transformed, \n",
    "    label_int, \n",
    "    random_state=42, \n",
    "    stratify=label_int\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = \\\n",
    "train_test_split(\n",
    "    df_train, \n",
    "    df_y_train, \n",
    "    random_state=42, \n",
    "    stratify=df_y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = MinMaxScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_val = ss.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=200)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_val = pca.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "print(f'Training data shape {X_train.shape}')\n",
    "\n",
    "# X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "print(f'Testing data shape {X_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(\n",
    "    eps = .2,\n",
    "    metric='euclidean', \n",
    "    min_samples = 5,\n",
    "    n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dbscan.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = confusion_matrix(y_val.astype(int), pred)\n",
    "print(f'TP - True Negative {cmat[0,0]}')\n",
    "print(f'FP - False Positive {cmat[0,1]}')\n",
    "print(f'FN - False Negative {cmat[1,0]}')\n",
    "print(f'TP - True Positive {cmat[1,1]}')\n",
    "print(f'Accuracy Rate: {np.divide(np.sum([cmat[0,0],cmat[1,1]]),np.sum(cmat))}')\n",
    "print(f'Misclassification Rate: {np.divide(np.sum([cmat[0,1],cmat[1,0]]),np.sum(cmat))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val.astype(int), pred, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_int = label.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(label_int)):\n",
    "    if label_int[i] == \"regular\":\n",
    "        label_int[i] = 0\n",
    "    elif label_int[i] == \"global\":\n",
    "        label_int[i] = 1\n",
    "    elif label_int[i] == \"local\":\n",
    "        label_int[i] = 1\n",
    "    else:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, df_y_train, df_y_test = \\\n",
    "train_test_split(\n",
    "    ori_subset_transformed, \n",
    "    label_int, \n",
    "    random_state=42, \n",
    "    stratify=label_int\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = MinMaxScaler()\n",
    "df_train = ss.fit_transform(df_train)\n",
    "df_test = ss.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=200)\n",
    "df_train = pca.fit_transform(df_train)\n",
    "df_test = pca.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "print(f'Training data shape {df_train.shape}')\n",
    "\n",
    "# X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "print(f'Testing data shape {df_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(\n",
    "    eps = .2,\n",
    "    metric='euclidean', \n",
    "    min_samples = 5,\n",
    "    n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.fit(df_train, df_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dbscan.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = confusion_matrix(df_y_test.astype(int), pred)\n",
    "print(f'TP - True Negative {cmat[0,0]}')\n",
    "print(f'FP - False Positive {cmat[0,1]}')\n",
    "print(f'FN - False Negative {cmat[1,0]}')\n",
    "print(f'TP - True Positive {cmat[1,1]}')\n",
    "print(f'Accuracy Rate: {np.divide(np.sum([cmat[0,0],cmat[1,1]]),np.sum(cmat))}')\n",
    "print(f'Misclassification Rate: {np.divide(np.sum([cmat[0,1],cmat[1,0]]),np.sum(cmat))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(df_y_test.astype(int), pred, digits=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
