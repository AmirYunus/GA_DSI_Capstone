{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>TECHNICAL REPORT - FINANCIAL FORENSIC DATA ANALYSIS<span class=\"tocSkip\"></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Amir Yunus<br>\n",
    "GitHub: https://github.com/AmirYunus/GA_DSI_Capstone\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREFACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: Fraud and the Accounting System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Association of Certified Fraud Examiners estimates in its Global Study on Occupational Fraud and Abuse, 2018 [[1]](../documents/2018-report-to-the-nations.pdf) that organisations lose 5% of their annual revenues to fraud. In their report, the term **\"occupational fraud\"** refers to, \n",
    "\n",
    ">_\" . . . an attack against the organisation from within, by the very people who were entrusted to protect its assets and resources\"_.\n",
    "\n",
    "A similar study, conducted by the auditors of PwC [[2]](../documents/global-economic-crime-and-fraud-survey-2018.pdf), revealed that 30% of the study respondents experienced losses of between USD 100,000 and USD 5 million in the last 24 months (as of 2018). The study also showed that financial statement fraud caused by far the greatest median loss of the surveyed fraud schemes.\n",
    "\n",
    "At the same time organizations accelerate the digitisation and reconfiguration of business processes [[3]](../documents/accelerating_the_digitization_of_business_processes.pdf) affecting in particular Accounting Information Systems (AIS) or more general Enterprise Resource Planning (ERP) systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"height: auto\" src=\"../images/accounting.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 1:** Hierarchical view of an Accounting Information System (AIS) that records distinct layers of abstraction, namely (1) the business process information, (2) the accounting information as well as the (3) technical journal entry information in designated database tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steadily, these systems collect vast quantities of electronic evidence at an almost atomic level. This holds in particular for the journal entries of an organization recorded in its general ledger and sub-ledger accounts. SAP, one of the most prominent ERP software providers, estimates that approx. 76% of the world's transaction revenue touches one of their systems [5].\n",
    "\n",
    "The illustration in **Figure 1** depicts a hierarchical view of an Accounting Information System (AIS) recording process and journal entry information in designated database tables. In the context of fraud examinations, the data collected by such systems may contain valuable traces of a potential fraud scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of Financial Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When conducting a detailed examination of real-world journal entries, usually recorded in large-scaled AIS or ERP systems, two prevalent characteristics can be observed:\n",
    "\n",
    "> - specific transactions attributes exhibit **a high variety of distinct attribute values** e.g. customer information, posted sub-ledgers, amount information, and \n",
    "> - the transactions exhibit **strong dependencies between specific attribute values** e.g. between customer information and type of payment, posting type and general ledgers. \n",
    "\n",
    "Derived from this observation we distinguish two classes of anomalous journal entries, namely **\"global\"** and **\"local\" anomalies** as illustrated in **Figure 2** below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"height: auto\" src=\"../images/anomalies.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 2:** Illustrative example of global and local anomalies portrait in a feature space of the two transaction features \"Posting Amount\" (Feature 1) and \"Posting Positions\" (Feature 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Global Anomalies***, are financial transactions that exhibit **unusual or rare individual attribute values**. These anomalies usually relate to highly skewed attributes e.g. seldom posting users, rarely used ledgers, or unusual posting times. \n",
    "\n",
    "Traditionally \"red-flag\" tests, performed by auditors during annual audits, are designed to capture those types of anomalies. However, such tests might result in a high volume of false positive alerts due to e.g. regular reverse postings, provisions and year-end adjustments usually associated with a low fraud risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Local Anomalies***, are financial transactions that exhibit an **unusual or rare combination of attribute values** while the individual attribute values occur quite frequently e.g. unusual accounting records. \n",
    "\n",
    "This type of anomaly is significantly more difficult to detect since perpetrators intend to disguise their activities trying to imitate a regular behaviour. As a result, such anomalies usually pose a high fraud risk since they might correspond to e.g. misused user accounts, irregular combinations of general ledger accounts and posting keys that don't follow an usual activity pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this lab is to walk you through a deep learning based methodology that can be used to detect of global and local anomalies in financial datasets. The proposed method is based on the following assumptions: \n",
    "\n",
    ">1. the majority of financial transactions recorded within an organizations’ ERP-system relate to regular day-to-day business activities and perpetrators need to deviate from the ”regular” in order to conduct fraud,\n",
    ">2. such deviating behaviour will be recorded by a very limited number of financial transactions and their respective attribute values or combination of attribute values and we refer to such deviation as \"anomaly\".\n",
    "\n",
    "Concluding from these assumptions we can learn a model of regular journal entries with minimal ”harm” caused by the potential anomalous ones.\n",
    "\n",
    "In order to detect such anomalies, we will train deep autoencoder networks to learn a compressed but \"lossy\" model of regular transactions and their underlying posting pattern. Imposing a strong regularization onto the network hidden layers limits the networks' ability to memorize the characteristics of anomalous journal entries. Once the training process is completed, the network will be able to reconstruct regular journal entries, while failing to do so for the anomalous ones.\n",
    "\n",
    "After completing the lab you should be familiar with:\n",
    "\n",
    ">1. the basic concepts, intuitions and major building blocks of autoencoder neural networks,\n",
    ">2. the techniques of pre-processing financial data in order to learn a model of its characteristics,\n",
    ">3. the application of autoencoder neural networks to detect anomalies in large-scale financial data, and,\n",
    ">4. the interpretation of the detection results of the networks as well as its reconstruction loss. \n",
    "\n",
    "Please note, that this lab is neither a complete nor comprehensive forensic data analysis approach or fraud examination strategy. However, the methodology and code provided in this lab can be modified or adapted to detect anomalous records in a variety of financial datasets. Subsequently, the detected records might serve as a starting point for a more detailed and substantive examination by auditors or compliance personnel. \n",
    "\n",
    "For this lab, we assume that you are familiar with the general concepts of deep neural networks (DNN) and GPUs as well as PyTorch and Python. For more information on these concepts please check the relevant labs of NVIDIA's Deep Learning Institute (DLI). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about potential fraud scenarios of your organization:\n",
    "\n",
    ">1. What scenarios or fraudulent activities you could think of? [3 min]\n",
    ">2. What data sources might affect or record those potential fraudulent activities? [5 min]\n",
    ">3. What kind of data analytics techniques could be applied to detect those activities? [5 min]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#PREFACE\" data-toc-modified-id=\"PREFACE-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>PREFACE</a></span><ul class=\"toc-item\"><li><span><a href=\"#Background:-Fraud-and-the-Accounting-System\" data-toc-modified-id=\"Background:-Fraud-and-the-Accounting-System-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Background: Fraud and the Accounting System</a></span></li><li><span><a href=\"#Classification-of-Financial-Anomalies\" data-toc-modified-id=\"Classification-of-Financial-Anomalies-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Classification of Financial Anomalies</a></span></li><li><span><a href=\"#Executive-Summary\" data-toc-modified-id=\"Executive-Summary-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Executive Summary</a></span></li><li><span><a href=\"#Content\" data-toc-modified-id=\"Content-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Content</a></span></li><li><span><a href=\"#Data-Dictionary\" data-toc-modified-id=\"Data-Dictionary-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Data Dictionary</a></span></li><li><span><a href=\"#Libraries\" data-toc-modified-id=\"Libraries-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Libraries</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Functions</a></span></li></ul></li><li><span><a href=\"#ABOUT-THE-DATASET\" data-toc-modified-id=\"ABOUT-THE-DATASET-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>ABOUT THE DATASET</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importing-the-Dataset\" data-toc-modified-id=\"Importing-the-Dataset-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Importing the Dataset</a></span></li></ul></li><li><span><a href=\"#EXPLORATORY-DATA-ANALYSIS\" data-toc-modified-id=\"EXPLORATORY-DATA-ANALYSIS-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>EXPLORATORY DATA ANALYSIS</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Attributes\" data-toc-modified-id=\"Data-Attributes-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Data Attributes</a></span><ul class=\"toc-item\"><li><span><a href=\"#BELNR\" data-toc-modified-id=\"BELNR-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>BELNR</a></span></li><li><span><a href=\"#WAERS\" data-toc-modified-id=\"WAERS-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>WAERS</a></span></li><li><span><a href=\"#BUKRS\" data-toc-modified-id=\"BUKRS-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>BUKRS</a></span></li><li><span><a href=\"#KTOSL\" data-toc-modified-id=\"KTOSL-3.1.4\"><span class=\"toc-item-num\">3.1.4&nbsp;&nbsp;</span>KTOSL</a></span></li><li><span><a href=\"#PRCTR\" data-toc-modified-id=\"PRCTR-3.1.5\"><span class=\"toc-item-num\">3.1.5&nbsp;&nbsp;</span>PRCTR</a></span></li><li><span><a href=\"#BSCHL\" data-toc-modified-id=\"BSCHL-3.1.6\"><span class=\"toc-item-num\">3.1.6&nbsp;&nbsp;</span>BSCHL</a></span></li><li><span><a href=\"#HKONT\" data-toc-modified-id=\"HKONT-3.1.7\"><span class=\"toc-item-num\">3.1.7&nbsp;&nbsp;</span>HKONT</a></span></li><li><span><a href=\"#DMBTR\" data-toc-modified-id=\"DMBTR-3.1.8\"><span class=\"toc-item-num\">3.1.8&nbsp;&nbsp;</span>DMBTR</a></span></li><li><span><a href=\"#WRBTR\" data-toc-modified-id=\"WRBTR-3.1.9\"><span class=\"toc-item-num\">3.1.9&nbsp;&nbsp;</span>WRBTR</a></span></li><li><span><a href=\"#label\" data-toc-modified-id=\"label-3.1.10\"><span class=\"toc-item-num\">3.1.10&nbsp;&nbsp;</span>label</a></span></li></ul></li><li><span><a href=\"#Evaluating-Dataset-Using-Benford's-Law\" data-toc-modified-id=\"Evaluating-Dataset-Using-Benford's-Law-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Evaluating Dataset Using Benford's Law</a></span></li><li><span><a href=\"#Baseline-Score\" data-toc-modified-id=\"Baseline-Score-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Baseline Score</a></span></li><li><span><a href=\"#Remove-Label\" data-toc-modified-id=\"Remove-Label-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Remove Label</a></span></li><li><span><a href=\"#Categorical-Features\" data-toc-modified-id=\"Categorical-Features-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Categorical Features</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Feature Engineering</a></span></li></ul></li><li><span><a href=\"#Numerical-Features\" data-toc-modified-id=\"Numerical-Features-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Numerical Features</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-3.6.1\"><span class=\"toc-item-num\">3.6.1&nbsp;&nbsp;</span>Feature Engineering</a></span></li></ul></li><li><span><a href=\"#Merge-Features\" data-toc-modified-id=\"Merge-Features-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Merge Features</a></span></li></ul></li><li><span><a href=\"#Adversarial-Autoencoder-Neural-Network\" data-toc-modified-id=\"Adversarial-Autoencoder-Neural-Network-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Adversarial Autoencoder Neural Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#AAE-Implementation---Encoder-$q_{\\theta}(z|x)$\" data-toc-modified-id=\"AAE-Implementation---Encoder-$q_{\\theta}(z|x)$-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>AAE Implementation - Encoder $q_{\\theta}(z|x)$</a></span></li><li><span><a href=\"#AAE-Implementation---Decoder-$p_{\\theta}(x|z)$\" data-toc-modified-id=\"AAE-Implementation---Decoder-$p_{\\theta}(x|z)$-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>AAE Implementation - Decoder $p_{\\theta}(x|z)$</a></span></li><li><span><a href=\"#AAE-Implementation---Discriminator-Network-$d_{\\theta}(z)$\" data-toc-modified-id=\"AAE-Implementation---Discriminator-Network-$d_{\\theta}(z)$-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>AAE Implementation - Discriminator Network $d_{\\theta}(z)$</a></span><ul class=\"toc-item\"><li><span><a href=\"#Adversarial-Autoencoder-Neural-Network-Training\" data-toc-modified-id=\"Adversarial-Autoencoder-Neural-Network-Training-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Adversarial Autoencoder Neural Network Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Reconstruction-Phase-Parameter\" data-toc-modified-id=\"Reconstruction-Phase-Parameter-4.3.1.1\"><span class=\"toc-item-num\">4.3.1.1&nbsp;&nbsp;</span>Reconstruction Phase Parameter</a></span></li><li><span><a href=\"#Regularization-Phase-Parameter\" data-toc-modified-id=\"Regularization-Phase-Parameter-4.3.1.2\"><span class=\"toc-item-num\">4.3.1.2&nbsp;&nbsp;</span>Regularization Phase Parameter</a></span></li></ul></li><li><span><a href=\"#Creation-of-the-Imposed-Latent-Prior-Distribution-$p(z)$\" data-toc-modified-id=\"Creation-of-the-Imposed-Latent-Prior-Distribution-$p(z)$-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Creation of the Imposed Latent Prior Distribution $p(z)$</a></span></li></ul></li><li><span><a href=\"#Training-the-Adversarial-Autoencoder-Neural-Network-(AAE)-Model\" data-toc-modified-id=\"Training-the-Adversarial-Autoencoder-Neural-Network-(AAE)-Model-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Training the Adversarial Autoencoder Neural Network (AAE) Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preparing-the-Network-Training-Data\" data-toc-modified-id=\"Preparing-the-Network-Training-Data-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Preparing the Network Training Data</a></span></li><li><span><a href=\"#Running-the-Network-Training\" data-toc-modified-id=\"Running-the-Network-Training-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>Running the Network Training</a></span></li></ul></li></ul></li><li><span><a href=\"#Evaluating-the-Autoencoder-Neural-Network-(AENN)-Model\" data-toc-modified-id=\"Evaluating-the-Autoencoder-Neural-Network-(AENN)-Model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Evaluating the Autoencoder Neural Network (AENN) Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Visualize-the-Latent-Space-Representation\" data-toc-modified-id=\"Visualize-the-Latent-Space-Representation-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Visualize the Latent Space Representation</a></span></li><li><span><a href=\"#Determine-Normalized-Mode-Divergence-(MD)-of-Each-Journal-Entry\" data-toc-modified-id=\"Determine-Normalized-Mode-Divergence-(MD)-of-Each-Journal-Entry-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Determine Normalized Mode Divergence (MD) of Each Journal Entry</a></span></li><li><span><a href=\"#Determine-Normalized-Reconstruction-Error-(RE)-of-Each-Journal-Entry\" data-toc-modified-id=\"Determine-Normalized-Reconstruction-Error-(RE)-of-Each-Journal-Entry-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Determine Normalized Reconstruction Error (RE) of Each Journal Entry</a></span></li><li><span><a href=\"#Determine-Anomaly-Score-$AS^{\\tau}$-of-Each-Journal-Entry\" data-toc-modified-id=\"Determine-Anomaly-Score-$AS^{\\tau}$-of-Each-Journal-Entry-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Determine Anomaly Score $AS^{\\tau}$ of Each Journal Entry</a></span></li><li><span><a href=\"#Visual-Inspection-of-the-Obtained-Anomaly-Scores\" data-toc-modified-id=\"Visual-Inspection-of-the-Obtained-Anomaly-Scores-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Visual Inspection of the Obtained Anomaly Scores</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utilities\n",
    "import os, sys, random, io, urllib\n",
    "import IPython\n",
    "from IPython.display import display, Markdown\n",
    "from datetime import datetime\n",
    "\n",
    "# import data science libraries\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import numpy as np\n",
    "\n",
    "# import pytorch libraries\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# import python plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log ():\n",
    "    now = str(datetime.now())\n",
    "    print(f'[LOG {now}]')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define encoder class\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    # define class constructor\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "\n",
    "        # call super class constructor\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # specify first layer - in 618, out 256\n",
    "        self.map_L1 = nn.Linear(input_size, hidden_size[0], bias=True) # init linearity\n",
    "        nn.init.xavier_uniform_(self.map_L1.weight) # init weights according to [9]\n",
    "        nn.init.constant_(self.map_L1.bias, 0.0) # constant initialization of the bias\n",
    "        self.map_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
    "\n",
    "        # specify second layer - in 256, out 64\n",
    "        self.map_L2 = nn.Linear(hidden_size[0], hidden_size[1], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L2.weight)\n",
    "        nn.init.constant_(self.map_L2.bias, 0.0)\n",
    "        self.map_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify third layer - in 64, out 16\n",
    "        self.map_L3 = nn.Linear(hidden_size[1], hidden_size[2], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L3.weight)\n",
    "        nn.init.constant_(self.map_L3.bias, 0.0)\n",
    "        self.map_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify fourth layer - in 16, out 4\n",
    "        self.map_L4 = nn.Linear(hidden_size[2], hidden_size[3], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L4.weight)\n",
    "        nn.init.constant_(self.map_L4.bias, 0.0)\n",
    "        self.map_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify fifth layer - in 4, out 2\n",
    "        self.map_L5 = nn.Linear(hidden_size[3], hidden_size[4], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L5.weight)\n",
    "        nn.init.constant_(self.map_L5.bias, 0.0)\n",
    "        self.map_R5 = torch.nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "        \n",
    "    # define forward pass\n",
    "    def forward(self, x):\n",
    "\n",
    "        # run forward pass through the network\n",
    "        x = self.map_R1(self.map_L1(x))\n",
    "        x = self.map_R2(self.map_L2(x))\n",
    "        x = self.map_R3(self.map_L3(x))\n",
    "        x = self.map_R4(self.map_L4(x))\n",
    "        x = self.map_R5(self.map_L5(x))\n",
    "\n",
    "        # return result\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define decoder class\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    # define class constructor\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "\n",
    "        # call super class constructor\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # specify first layer - in 2, out 4\n",
    "        self.map_L1 = nn.Linear(hidden_size[0], hidden_size[1], bias=True) # init linearity\n",
    "        nn.init.xavier_uniform_(self.map_L1.weight) # init weights according to [9]\n",
    "        nn.init.constant_(self.map_L1.bias, 0.0) # constant initialization of the bias\n",
    "        self.map_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
    "\n",
    "        # specify second layer - in 4, out 16\n",
    "        self.map_L2 = nn.Linear(hidden_size[1], hidden_size[2], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L2.weight)\n",
    "        nn.init.constant_(self.map_L2.bias, 0.0)\n",
    "        self.map_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify third layer - in 16, out 64\n",
    "        self.map_L3 = nn.Linear(hidden_size[2], hidden_size[3], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L3.weight)\n",
    "        nn.init.constant_(self.map_L3.bias, 0.0)\n",
    "        self.map_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify fourth layer - in 64, out 256\n",
    "        self.map_L4 = nn.Linear(hidden_size[3], hidden_size[4], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L4.weight)\n",
    "        nn.init.constant_(self.map_L4.bias, 0.0)\n",
    "        self.map_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "        \n",
    "        # specify fifth layer - in 256, out 618\n",
    "        self.map_L5 = nn.Linear(hidden_size[4], output_size, bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L5.weight)\n",
    "        nn.init.constant_(self.map_L5.bias, 0.0)\n",
    "        self.map_S5 = torch.nn.Sigmoid()\n",
    "\n",
    "    # define forward pass\n",
    "    def forward(self, x):\n",
    "\n",
    "        # run forward pass through the network\n",
    "        x = self.map_R1(self.map_L1(x))\n",
    "        x = self.map_R2(self.map_L2(x))\n",
    "        x = self.map_R3(self.map_L3(x))\n",
    "        x = self.map_R4(self.map_L4(x))\n",
    "        x = self.map_S5(self.map_L5(x))\n",
    "\n",
    "        # return result\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    # define class constructor\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "\n",
    "        # call super class constructor\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # specify first layer - in 2, out 256\n",
    "        self.map_L1 = nn.Linear(input_size, hidden_size[0], bias=True) # init linearity\n",
    "        nn.init.xavier_uniform_(self.map_L1.weight) # init weights according to [9]\n",
    "        nn.init.constant_(self.map_L1.bias, 0.0) # constant initialization of the bias\n",
    "        self.map_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
    "\n",
    "        # specify second layer - in 256, out 16\n",
    "        self.map_L2 = nn.Linear(hidden_size[0], hidden_size[1], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L2.weight)\n",
    "        nn.init.constant_(self.map_L2.bias, 0.0)\n",
    "        self.map_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify third layer - in 16, out 4\n",
    "        self.map_L3 = nn.Linear(hidden_size[1], hidden_size[2], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L3.weight)\n",
    "        nn.init.constant_(self.map_L3.bias, 0.0)\n",
    "        self.map_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "        \n",
    "        # specify fourth layer - in 4, out 2\n",
    "        self.map_L4 = nn.Linear(hidden_size[2], output_size, bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L4.weight)\n",
    "        nn.init.constant_(self.map_L4.bias, 0.0)\n",
    "        self.map_S4 = torch.nn.Sigmoid()\n",
    "\n",
    "    # define forward pass\n",
    "    def forward(self, x):\n",
    "\n",
    "        # run forward pass through the network\n",
    "        x = self.map_R1(self.map_L1(x))\n",
    "        x = self.map_R2(self.map_L2(x))\n",
    "        x = self.map_R3(self.map_L3(x))\n",
    "        x = self.map_S4(self.map_L4(x))\n",
    "\n",
    "        # return result\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define euclidean distance calculation\n",
    "def compute_euclid_distance(x, y):\n",
    "    \n",
    "    # calculate euclidean distance \n",
    "    euclidean_distance = np.sqrt(np.sum((x - y) ** 2, axis=1))\n",
    "    \n",
    "    # return euclidean distance\n",
    "    return euclidean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init deterministic seed\n",
    "seed_value = 1234\n",
    "rd.seed(seed_value) # set random seed\n",
    "np.random.seed(seed_value) # set numpy seed\n",
    "torch.manual_seed(seed_value) # set pytorch seed CPU\n",
    "if (torch.backends.cudnn.version() != None and USE_CUDA == True):\n",
    "    torch.cuda.manual_seed(seed_value) # set pytorch seed GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../data'): os.makedirs('../data')  # create data directory\n",
    "if not os.path.exists('../models'): os.makedirs('../models')  # create trained models directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABOUT THE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will conduct a descriptive analysis of the labs financial dataset. Furthermore, we will apply some necessary pre-processing steps to train a deep neural network. The lab is based on a derivation of the **\"Synthetic Financial Dataset For Fraud Detection\"** by Lopez-Rojas [6] available via the Kaggle predictive modelling and analytics competitions platform that can be obtained using the following link: https://www.kaggle.com/ntnu-testimon/paysim1.\n",
    "\n",
    "Let's start loading the dataset and investigate its structure and attributes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# load the dataset into the notebook kernel\n",
    "ori_dataset = pd.read_csv('../data/fraud_dataset_v2.csv')\n",
    "# inspect the datasets dimensionalities\n",
    "print(F'Transactional dataset of {ori_dataset.shape[0]} rows and {ori_dataset.shape[1]} columns loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We augmented the dataset and renamed the attributes to appear more similar to a real-world dataset that one usually observes in SAP-ERP systems as part of SAP's Finance and Cost controlling (FICO) module. \n",
    "\n",
    "The dataset contains a subset of in total 7 categorical and 2 numerical attributes available in the FICO BKPF (containing the posted journal entry headers) and BSEG (containing the posted journal entry segments) tables. Please, find below a list of the individual attributes as well as a brief description of their respective semantics:\n",
    "\n",
    ">- `BELNR`: the accounting document number,\n",
    ">- `BUKRS`: the company code,\n",
    ">- `BSCHL`: the posting key,\n",
    ">- `HKONT`: the posted general ledger account,\n",
    ">- `PRCTR`: the posted profit center,\n",
    ">- `WAERS`: the currency key,\n",
    ">- `KTOSL`: the general ledger account key,\n",
    ">- `DMBTR`: the amount in local currency,\n",
    ">- `WRBTR`: the amount in document currency.\n",
    "\n",
    "Let's also have a closer look into the top 10 rows of the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect top rows of dataset\n",
    "log()\n",
    "ori_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BELNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# Display object type and values for each feature\n",
    "display(Markdown(f'<b>{ori_dataset.columns[0]}</b> - {ori_dataset.BELNR.dtype}'))\n",
    "display(Markdown(f'Values:'))\n",
    "print(f'{ori_dataset.BELNR.value_counts(normalize=True)}\\n')\n",
    "print()\n",
    "n_nan = ori_dataset.BELNR.isnull().sum()\n",
    "if n_nan > 0:\n",
    "    print(f'{ori_dataset.Date} has {n_nan} NaNs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WAERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# Display object type and values for each feature\n",
    "display(Markdown(f'<b>{ori_dataset.columns[1]}</b> - {ori_dataset.WAERS.dtype}'))\n",
    "display(Markdown(f'Values:'))\n",
    "print(f'{ori_dataset.WAERS.value_counts(normalize=True)}\\n')\n",
    "print()\n",
    "n_nan = ori_dataset.WAERS.isnull().sum()\n",
    "if n_nan > 0:\n",
    "    print(f'{ori_dataset.WAERS} has {n_nan} NaNs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "ori_dataset['WAERS'].value_counts().plot(kind='bar',)\n",
    "plt.title('Frequency of Observations in WAERS Feature',{'size': 15})\n",
    "plt.xlabel('Observations', {'size': 15})\n",
    "plt.ylabel('Frequency', {'size': 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUKRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log()\n",
    "# Display object type and values for each feature\n",
    "display(Markdown(f'<b>{ori_dataset.columns[2]}</b> - {ori_dataset.BUKRS.dtype}'))\n",
    "display(Markdown(f'Values:'))\n",
    "print(f'{ori_dataset.BUKRS.value_counts(normalize=True)}\\n')\n",
    "print()\n",
    "n_nan = ori_dataset.BUKRS.isnull().sum()\n",
    "if n_nan > 0:\n",
    "    print(f'{ori_dataset.BUKRS} has {n_nan} NaNs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "ori_dataset['BUKRS'].value_counts().plot(kind='bar',)\n",
    "plt.title('Frequency of Observations in BUKRS Feature',{'size': 15})\n",
    "plt.xlabel('Observations', {'size': 15})\n",
    "plt.ylabel('Frequency', {'size': 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KTOSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# Display object type and values for each feature\n",
    "display(Markdown(f'<b>{ori_dataset.columns[3]}</b> - {ori_dataset.KTOSL.dtype}'))\n",
    "display(Markdown(f'Values:'))\n",
    "print(f'{ori_dataset.KTOSL.value_counts(normalize=True)}\\n')\n",
    "print()\n",
    "n_nan = ori_dataset.KTOSL.isnull().sum()\n",
    "if n_nan > 0:\n",
    "    print(f'{ori_dataset.KTOSL} has {n_nan} NaNs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "ori_dataset['KTOSL'].value_counts().plot(kind='bar',)\n",
    "plt.title('Frequency of Observations in KTOSL Feature',{'size': 15})\n",
    "plt.xlabel('Observations', {'size': 15})\n",
    "plt.ylabel('Frequency', {'size': 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRCTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# Display object type and values for each feature\n",
    "display(Markdown(f'<b>{ori_dataset.columns[4]}</b> - {ori_dataset.PRCTR.dtype}'))\n",
    "display(Markdown(f'Values:'))\n",
    "print(f'{ori_dataset.PRCTR.value_counts(normalize=True)}\\n')\n",
    "print()\n",
    "n_nan = ori_dataset.PRCTR.isnull().sum()\n",
    "if n_nan > 0:\n",
    "    print(f'{ori_dataset.PRCTR} has {n_nan} NaNs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "ori_dataset['PRCTR'].value_counts().plot(kind='bar',)\n",
    "plt.title('Frequency of Observations in PRCTR Feature',{'size': 15})\n",
    "plt.xlabel('Observations', {'size': 15})\n",
    "plt.ylabel('Frequency', {'size': 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BSCHL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# Display object type and values for each feature\n",
    "display(Markdown(f'<b>{ori_dataset.columns[5]}</b> - {ori_dataset.BSCHL.dtype}'))\n",
    "display(Markdown(f'Values:'))\n",
    "print(f'{ori_dataset.BSCHL.value_counts(normalize=True)}\\n')\n",
    "print()\n",
    "n_nan = ori_dataset.BSCHL.isnull().sum()\n",
    "if n_nan > 0:\n",
    "    print(f'{ori_dataset.BSCHL} has {n_nan} NaNs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "ori_dataset['BSCHL'].value_counts().plot(kind='bar',)\n",
    "plt.title('Frequency of Observations in BSCHL Feature',{'size': 15})\n",
    "plt.xlabel('Observations', {'size': 15})\n",
    "plt.ylabel('Frequency', {'size': 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HKONT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# Display object type and values for each feature\n",
    "display(Markdown(f'<b>{ori_dataset.columns[6]}</b> - {ori_dataset.HKONT.dtype}'))\n",
    "display(Markdown(f'Values:'))\n",
    "print(f'{ori_dataset.HKONT.value_counts(normalize=True)}\\n')\n",
    "print()\n",
    "n_nan = ori_dataset.HKONT.isnull().sum()\n",
    "if n_nan > 0:\n",
    "    print(f'{ori_dataset.HKONT} has {n_nan} NaNs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "ori_dataset['HKONT'].value_counts().plot(kind='bar',)\n",
    "plt.title('Frequency of Observations in HKONT Feature',{'size': 15})\n",
    "plt.xlabel('Observations', {'size': 15})\n",
    "plt.ylabel('Frequency', {'size': 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DMBTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# Display object type and values for each feature\n",
    "display(Markdown(f'<b>{ori_dataset.columns[7]}</b> - {ori_dataset.DMBTR.dtype}'))\n",
    "display(Markdown(f'Values:'))\n",
    "print(f'{ori_dataset.DMBTR.value_counts(normalize=True)}\\n')\n",
    "print()\n",
    "n_nan = ori_dataset.DMBTR.isnull().sum()\n",
    "if n_nan > 0:\n",
    "    print(f'{ori_dataset.DMBTR} has {n_nan} NaNs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "ori_dataset['DMBTR'].plot(kind='hist',)\n",
    "plt.title('Frequency of Observations in DMBTR Feature',{'size': 15})\n",
    "plt.xlabel('Observations', {'size': 15})\n",
    "plt.ylabel('Frequency', {'size': 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WRBTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# Display object type and values for each feature\n",
    "display(Markdown(f'<b>{ori_dataset.columns[8]}</b> - {ori_dataset.WRBTR.dtype}'))\n",
    "display(Markdown(f'Values:'))\n",
    "print(f'{ori_dataset.WRBTR.value_counts(normalize=True)}\\n')\n",
    "print()\n",
    "n_nan = ori_dataset.WRBTR.isnull().sum()\n",
    "if n_nan > 0:\n",
    "    print(f'{ori_dataset.WRBTR} has {n_nan} NaNs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "ori_dataset['WRBTR'].plot(kind='hist',)\n",
    "plt.title('Frequency of Observations in WRBTR Feature',{'size': 15})\n",
    "plt.xlabel('Observations', {'size': 15})\n",
    "plt.ylabel('Frequency', {'size': 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also have noticed the attribute `label` in the data. We will use this field throughout the lab to evaluate the quality of our trained models. The field describes the true nature of each individual transaction of either being a **regular** transaction (denoted by `regular`) or an **anomaly** (denoted by `global` and `local`). Let's have closer look into the distribution of the regular vs. anomalous transactions in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of anomalies vs. regular transactions\n",
    "log()\n",
    "ori_dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# Display object type and values for each feature\n",
    "display(Markdown(f'<b>{ori_dataset.columns[9]}</b> - {ori_dataset.label.dtype}'))\n",
    "display(Markdown(f'Values:'))\n",
    "print(f'{ori_dataset.label.value_counts(normalize=True)}\\n')\n",
    "print()\n",
    "n_nan = ori_dataset.label.isnull().sum()\n",
    "if n_nan > 0:\n",
    "    print(f'{ori_dataset.label} has {n_nan} NaNs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the statistic reveals that, similar to real world scenarios, we are facing a highly \"unbalanced\" dataset. Overall, the dataset contains only a small fraction of **100 (0.018%)** anomalous transactions. While the 100 anomalous entries encompass **70 (0.013%)** \"global\" anomalies and **30 (0.005%)** \"local\" anomalies as introduced in section 1.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Dataset Using Benford's Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benford's law is an observation about the frequency distribution of leading digits in many real-life sets of numerical data. The law states that in many naturally occurring collections of numbers, the leading significant digit is likely to be small. For example, in sets that obey the law, the number 1 appears as the most significant digit about 30% of the time, while 9 appears as the most significant digit less than 5% of the time. If the digits were distributed uniformly, they would each occur about 11.1% of the time. Benford's law also makes predictions about the distribution of second digits, third digits, digit combinations, and so on. - From [Wikipedia](https://en.wikipedia.org/wiki/Benford%27s_law)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# make a copy of original dataset\n",
    "ori_dataset_bf = ori_dataset.copy()\n",
    "\n",
    "# map out the first digit and display top 5 rows\n",
    "ori_dataset_bf['FIRST_DIGIT'] = ori_dataset_bf.DMBTR.map(lambda a: str(a)[0]).astype(int)\n",
    "ori_dataset_bf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# display the actual percentage distribution of dataset\n",
    "actuals = ori_dataset_bf.FIRST_DIGIT.value_counts(normalize=True).sort_index()\n",
    "actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log()\n",
    "# calculate the expected distribution based on Benford's Law\n",
    "digits = list(range(1,10))\n",
    "benford = [np.log10(1 + 1/d) for d in digits]\n",
    "plt.figure(figsize = (16,9))\n",
    "\n",
    "# plot graph to visualise distribution\n",
    "plt.bar(digits, benford, label='Exptected')\n",
    "plt.plot(actuals, color='r', label='Actual')\n",
    "plt.xticks(digits)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No obvious anomalies. Digits starting with `2` may have issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# display the distribution of values with starting with number 2\n",
    "arr = ori_dataset_bf[(ori_dataset_bf['FIRST_DIGIT'] == 2)]\n",
    "arr.DMBTR.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log()\n",
    "arr.DMBTR.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to determine anomalies using Benford's Law. We will proceed with Adversarial Auto-Encoder Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of anomalies vs. regular transactions\n",
    "log()\n",
    "ori_dataset.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the \"ground-truth\" label information for the following steps of the lab\n",
    "log()\n",
    "label = ori_dataset.pop('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the initial data assessment above we can observe that the majority of attributes recorded in AIS- and ERP-systems correspond to categorical (discrete) attribute values, e.g. the posting date, the general-ledger account, the posting type, the currency. Let's have a more detailed look into the distribution of two dataset attributes, namely (1) the posting key `BSCHL` as well as (2) the general ledger account `HKONT`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to plot posting key and general ledger account side by side\n",
    "log()\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_figwidth(16)\n",
    "fig.set_figheight(9)\n",
    "\n",
    "# plot the distribution of the posting key attribute\n",
    "g = sns.countplot(x=ori_dataset.loc[label=='regular', 'BSCHL'], ax=ax[0])\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=0)\n",
    "g.set_title('Distribution of BSCHL attribute values')\n",
    "\n",
    "# plot the distribution of the general ledger account attribute\n",
    "g = sns.countplot(x=ori_dataset.loc[label=='regular', 'HKONT'], ax=ax[1])\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=0)\n",
    "g.set_title('Distribution of HKONT attribute values');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, neural networks are in general not designed to be trained directly on categorical data and require the attributes to be trained on to be numeric. One simple way to meet this requirement is by applying a technique referred to as **\"one-hot\" encoding**. Using this encoding technique, we will derive a numerical representation of each of the categorical attribute values. One-hot encoding creates new binary columns for each categorical attribute value present in the original data. \n",
    "\n",
    "Let's work through a brief example: The **categorical attribute “Receiver”** below contains the names \"John\", \"Timur\" and \"Marco\". We \"one-hot\" encode the names by creating a separate binary column for each possible name value observable in the \"Receiver\" column. Now, we encode for each transaction that contains the value \"John\" in the \"Receiver\" column this observation with 1.0 in the newly created \"John\" column and 0.0 in all other created name columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 430px; height: auto\" src=\"../images/encoding.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this technique will \"one-hot\" encode the 6 categorical attributes in the original transactional dataset. This can be achieved using the `get_dummies()` function available in the Pandas data science library:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select categorical attributes to be \"one-hot\" encoded\n",
    "log()\n",
    "\n",
    "categorical_attr_names = ['KTOSL', 'PRCTR', 'BSCHL', 'HKONT', 'BUKRS', 'WAERS']\n",
    "\n",
    "# encode categorical attributes into a binary one-hot encoded representation \n",
    "ori_dataset_categ_transformed = pd.get_dummies(ori_dataset[categorical_attr_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's inspect the encoding of 10 sample transactions to see if we have been successfull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect encoded sample transactions\n",
    "log()\n",
    "ori_dataset_categ_transformed.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now inspect the distributions of the two numerical attributes contained in the transactional dataset namely, the (1) local currency amount `DMBTR` and the (2) document currency amount `WRBTR`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the plots\n",
    "log()\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_figwidth(16)\n",
    "fig.set_figheight(9)\n",
    "\n",
    "# plot distribution of the local amount attribute\n",
    "g = sns.distplot(ori_dataset['DMBTR'].tolist(), ax=ax[0])\n",
    "g.set_title('Distribution of DMBTR amount values')\n",
    "\n",
    "# set axis-labels \n",
    "ax[0].set_xlabel('DMBTR')\n",
    "ax[0].set_ylabel('density')\n",
    "\n",
    "# plot distribution of the document amount attribute\n",
    "g = sns.distplot(ori_dataset['WRBTR'].tolist(), ax=ax[1])\n",
    "g.set_title('Distribution of WRBTR amount values')\n",
    "\n",
    "# set axis-labels\n",
    "ax[1].set_xlabel('WRBTR')\n",
    "ax[1].set_ylabel('density');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it can be observed, that for both attributes the distributions of amount values are heavy tailed. In order to approach faster a potential global minimum scaling and normalization of numerical input values is good a practice. Therefore, we first log-scale both variables and second min-max normalize the scaled amounts to the interval [0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select \"DMBTR\" vs. \"WRBTR\" attribute\n",
    "log()\n",
    "\n",
    "numeric_attr_names = ['DMBTR', 'WRBTR']\n",
    "\n",
    "# add a small epsilon to eliminate zero values from data for log scaling\n",
    "numeric_attr = ori_dataset[numeric_attr_names] + 1e-7\n",
    "numeric_attr = numeric_attr.apply(np.log)\n",
    "\n",
    "# normalize all numeric attributes to the range [0,1]\n",
    "ori_dataset_numeric_attr = (numeric_attr - numeric_attr.min()) / (numeric_attr.max() - numeric_attr.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize the log-scaled and min-max normalized distributions of both attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the plots\n",
    "log()\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_figwidth(16)\n",
    "fig.set_figheight(9)\n",
    "\n",
    "# plot distribution of the local amount attribute\n",
    "g = sns.distplot(ori_dataset_numeric_attr['DMBTR'].tolist(), ax=ax[0])\n",
    "g.set_title('Distribution of scaled DMBTR amount values')\n",
    "\n",
    "# set axis-labels \n",
    "ax[0].set_xlabel('DMBTR')\n",
    "ax[0].set_ylabel('density')\n",
    "\n",
    "# plot distribution of the local amount attribute\n",
    "g = sns.distplot(ori_dataset_numeric_attr['WRBTR'].tolist(), ax=ax[1])\n",
    "g.set_title('Distribution of scaled WRBTR amount values')\n",
    "\n",
    "# set axis-labels\n",
    "ax[1].set_xlabel('WRBTR')\n",
    "ax[1].set_ylabel('density');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's now visually investigate the scaled distributions of both attributes in terms of the distinct anomaly classes contained in the population of journal entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append 'label' attribute \n",
    "log()\n",
    "\n",
    "numeric_attr_vis = ori_dataset_numeric_attr.copy()\n",
    "numeric_attr_vis['label'] = label\n",
    "\n",
    "# plot the log-scaled and min-max normalized numeric attributes\n",
    "g = sns.pairplot(data=numeric_attr_vis, vars=numeric_attr_names, hue='label', diag_kind='kde', palette={'regular': 'C0', 'local': 'C3', 'global': 'C1'}, markers=['o', 'x', 'x'])\n",
    "\n",
    "# set figure title\n",
    "g.fig.suptitle('Distribution of DMBTR vs. WRBTR amount values', y=1.02)\n",
    "\n",
    "# set figure size\n",
    "g.fig.set_size_inches(16, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, as anticipated the numeric attribute values of the \"global\" anomalies (orange) fall outside the range of the regular amount distributions due to their unusual high amount values. In contrast, the numeric attribute values of the \"local\" anomalies (red) are much more commingled within the regular transaction amounts.\n",
    "As DMBTR attribute contains a number of extreme values we might want to visulalize its distribution by omitting those set of extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we merge both pre-processed numerical and categorical attributes into a single dataset that we will use for training our deep autoencoder neural network (explained an implemented in the following section 4.)\n",
    "\n",
    "Now, let's again have a look at the dimensionality of the dataset after we applied the distinct pre-processing steps to the attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge categorical and numeric subsets\n",
    "ori_subset_transformed = pd.concat([ori_dataset_categ_transformed, ori_dataset_numeric_attr], axis = 1)\n",
    "# inspect final dimensions of pre-processed transactional data\n",
    "log ()\n",
    "ori_subset_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the pre-processing steps above you may have noticed, that we didn't encode the attributes `WAERS` and `BUKRS` yet. This we left as an exercise for you:\n",
    "\n",
    ">1. Plot and inspect the distribution of the values of both attributes `WAERS` and `BUKRS`. [3 min]\n",
    ">2. Encode both variables using the `get_dummies()` method provided by the Pandas library. [5 min]\n",
    ">3. Merge your encoding results with the Pandas `ori_subset_transformed` data frame. [5 min]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, upon completion of all the pre-processing steps (incl. the exercises) we should end up with an encoded dataset consisting of a total number of 533,009 records (rows) and **618 encoded attributes** (columns). Let's keep the number number of columns in mind since it will define the dimensionality of the input- and output-layer of our deep autoencoder network which we will now implement in the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, y_train, y_test = \\\n",
    "train_test_split(\n",
    "    ori_subset_transformed, \n",
    "    label, \n",
    "    random_state=42, \n",
    "    stratify=label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(label)):\n",
    "    if label[i] == \"regular\":\n",
    "        label[i] = 0\n",
    "    elif label[i] == \"global\":\n",
    "        label[i] = 1\n",
    "    elif label[i] == \"local\":\n",
    "        label[i] = 2\n",
    "    else:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(train)\n",
    "X_test = ss.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "print(f'Training data shape {X_train.shape}')\n",
    "\n",
    "# X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "print(f'Testing data shape {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Autoencoder Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Adversarial Autoencoder Neural Network (AAE) architecture, as illustrated in the figure below, extends the concept\n",
    "of Autoencoder Neural Networks (AE) by imposing an arbitrary prior on the AEs latent space using a GAN training setup. This is achieved by training the AAE jointly in two phases (1) a reconstruction phase as well as (2) an adversarial regularization phase.\n",
    "\n",
    "In the reconstruction phase, the AAEs encoder network $q_{\\theta}(z|x)$ is trained to learn an aggregated posterior distribution $q(z)$ of the journal entries $X$ over the latent code vector $Z$. Thereby, the learned posterior distribution corresponds to a compressed representation of the journal entry characteristics. Similarly to AENs, the decoder\n",
    "network $p_{\\theta}(\\hat{x}|z)$ of the AAE utilizes the learned latent code vector representations $Z$ to reconstruct the journal entries $\\hat{X}$ as faithfully as possible to minimize the AAEs reconstruction error.\n",
    "\n",
    "In the regularization phase, an adversarial training setup is applied were the encoder network $q_{\\theta}(z|x)$ of the AAE functions as the generator network. In addition, a discriminator network $d_{\\theta}(z)$ is attached on top of the learned latent code vector $Z$. Similarly to GANs, the discriminator network of the AAE is trained to distinguish samples of an imposed prior distribution $p(z)$ onto $Z$ from the learned aggregated posterior distribution $q(z)$. In contrast, the encoder network is trained to learn a posterior distribution $p(z) ≈ q(z)$ that fools the discriminator network into thinking that the samples drawn from $q(z)$ originate from the imposed prior distribution $p(z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 830px; height: auto\" src=\"../images/autoencoder_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AAE Implementation - Encoder $q_{\\theta}(z|x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start implementing an AAE by first implementing the encoder-generator network $q_{\\theta}(z|x)$ using PyTorch. For the encoder-generator, we aim to implement a network consisting of **five fully-connected layers**. Furthermore, the encoder-generator is specified by the following number of neurons per layer: \"618-256-64-16-4-2\". Meaning the first layer consists of 618 neurons (specified by the dimensionality of our input data), the second layer of 256 neurons and the subsequent layers of 64, 16, 4 and 2 neurons respectively.\n",
    "\n",
    "Some elements of the encoder network code below should be given particular attention:\n",
    "\n",
    ">- `self.encoder_Lx`: defines the linear transformation of the layer applied to the incoming input: $Wx + b$.\n",
    ">- `nn.init.xavier_uniform`: inits the layer weights using a uniform distribution according to [9].\n",
    ">- `nn.init.constant`: inits the layer bias with a constant value of 0.0. \n",
    ">- `self.encoder_Rx`: defines the non-linear transformation of the layer: $\\sigma(\\cdot)$.\n",
    "\n",
    "We use **\"Leaky ReLUs\"** as introduced by Xu et al. in [7] to avoid \"dying\" non-linearities and to speed up training convergence. Leaky ReLUs allow a small gradient even when a particular neuron is not active.\n",
    "\n",
    "Now, we are ready to instantiate the encoder-generator model to be trained on the CPU or to be trained on any of the available GPUs (if CUDNN is available and `USE_CUDA` is set to `True`) by execution of the following cell.\n",
    "\n",
    "Once the model is initialized we can visualize the model structure and review the implemented network architecture by execution of the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# init training network classes / architectures\n",
    "encoder_train = Encoder(input_size=X_train.shape[1], hidden_size=[256, 64, 16, 4, 2])\n",
    "\n",
    "# push to cuda if cudnn is available\n",
    "if (torch.backends.cudnn.version() != None and USE_CUDA == True):\n",
    "    encoder_train = encoder_train.cuda()\n",
    "\n",
    "print(f'Encoder Architecture:\\n\\n{encoder_train}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AAE Implementation - Decoder $p_{\\theta}(x|z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue the AAE by implementing the corresponding decoder network. The decoder also consists of five fully-connected layers. Furthermore, the decoder network is intended to **symmetrically mirror** the encoder networks architecture by a layer wise inversion \"2-4-16-64-256\" of the encoder network layers.\n",
    "\n",
    "Let's also instantiate the decoder model for CPU or GPU training and convince ourselves that it was successfully initialized by printing and reviewing its architecture by execution of the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# init training network classes / architectures\n",
    "decoder_train = Decoder(output_size=X_train.shape[1], hidden_size=[2, 4, 16, 64, 256])\n",
    "\n",
    "# push to cuda if cudnn is available\n",
    "if (torch.backends.cudnn.version() != None) and (USE_CUDA == True):\n",
    "    decoder_train = decoder_train.cuda()\n",
    "\n",
    "print(f'Decoder Architecture:\\n\\n{decoder_train}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AAE Implementation - Discriminator Network $d_{\\theta}(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's, now as a final step, complete the AAE implementation by implementing the discriminator network $d_{\\theta}(z)$. The discriminator also consists of five fully-connected layers. Furthermore, the discriminator is specified by the following number of neurons per layer: \"256-16-4-2\".\n",
    "\n",
    "Let's also instantiate the discriminator model for CPU or GPU training and convince ourselves that it was successfully initialized by printing and reviewing its architecture by execution of the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# init training network classes / architectures\n",
    "discriminator_train = Discriminator(input_size=2, hidden_size=[256, 16, 4, 2], output_size=1)\n",
    "\n",
    "# push to cuda if cudnn is available\n",
    "if (torch.backends.cudnn.version() != None) and (USE_CUDA == True):\n",
    "    discriminator_train = discriminator_train.cuda()\n",
    "\n",
    "print(f'Discriminator Architecture:\\n\\n{discriminator_train}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Autoencoder Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have implemented the AAE we are ready to train the network. Prior to starting the training, we need to define an appropriate loss functions, learning rates and parameter optimization techniques. Remember, we aim to train the adversarial autoencoder jointly in two training phases, namely (1) a reconstruction phase as well as (2) a regularization phase. In the following we will set the training parameters of each training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstruction Phase Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the reconstruction phase, the AAEs encoder network $q_{\\theta}(z|x)$ is trained to learn an aggregated posterior distribution $q(z)$ of the journal entries $X$ over the latent code vector $Z$. Thereby, the learned posterior distribution corresponds to a compressed representation of the journal entry characteristics. Similarly to AENs, the decoder network $p_{\\theta}(\\hat{x}|z)$ of the AAE utilizes the learned latent code vector representations $Z$ to reconstruct the journal entries $\\hat{X}$ as faithfully as possible to minimize the AAEs reconstruction error.\n",
    "\n",
    "To achieve this optimization objective, we calculate (1) the **binary cross-entropy reconstruction error (BCE)** of the categorical attribute value encodings $x^{i}_{cat}$, e.g., the encoded general ledger account ids, and (2) the **mean-squared reconstruction error (MSE)** of the numerical attribute value encodings $x^{i}_{con}$, e.g., the encoded posting amount, as given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>$\\mathcal{L}_{\\theta}^{REC}(x^{i};\\hat{x}^{i}) = \\gamma \\hspace{1mm} \\mathcal{L}^{CE}_{\\theta}(x^{i}_{cat};\\hat{x}^{i}_{cat}) + (1 - \\gamma) \\hspace{1mm} \\mathcal{L}^{MSE}_{\\theta}(x^{i}_{con};\\hat{x}^{i}_{con})$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for a set of $n$-journal entries $x^{i}$, $i=1,...,n$ and their respective reconstructions $\\hat{x}^{i}$ and all journal entry attributes $j=1,...,k$. Luckily, an implementation of the BCE and MSE loss is already available in PyTorch. It can be instantiated \"off-the-shelf\" via execution of the following PyTorch commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# define the optimization criterion / loss function\n",
    "reconstruction_criterion_categorical = nn.BCELoss(reduction='mean')\n",
    "reconstruction_criterion_numeric = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# push to cuda if cudnn is available\n",
    "if (torch.backends.cudnn.version() != None and USE_CUDA == True):\n",
    "    reconstruction_criterion_categorical = reconstruction_criterion_categorical.cuda()\n",
    "    reconstruction_criterion_numeric = reconstruction_criterion_numeric.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Adam optimization as proposed in [11] and set the learning-rate  l=0.001. Each mini-batch step the optimizer will update the encoder- and decoder-parameters $\\theta$ according to degree of reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# define encoder and decoded learning rate\n",
    "learning_rate_enc = 1e-3\n",
    "learning_rate_dec = 1e-3\n",
    "\n",
    "# define encoder and decoder optimization strategy\n",
    "encoder_optimizer = optim.Adam(encoder_train.parameters(), lr=learning_rate_enc)\n",
    "decoder_optimizer = optim.Adam(decoder_train.parameters(), lr=learning_rate_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization Phase Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the regularization phase, an adversarial training setup is applied were the encoder network $q_{\\theta}(z|x)$ of the AAE functions as the generator network. In addition, a discriminator network $d_{\\phi}(z)$ is attached on top of the learned latent code vector $Z$. Similarly to GANs, the discriminator network of the AAE is trained to distinguish samples of an imposed prior distribution $p(z)$ onto $Z$ from the learned aggregated posterior distribution $q(z)$. In contrast, the encoder network is trained to learn a posterior distribution $p(z) \\approx q(z)$ that fools the discriminator network into thinking that the samples drawn from $q(z)$ originate from the imposed prior distribution $p(z)$.\n",
    "\n",
    "To achieve this optimization objective, we calculate the **binary cross-entropy reconstruction error (BCE)**, as given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# init the discriminator losses\n",
    "discriminator_criterion = nn.BCELoss()\n",
    "\n",
    "# push to cuda if cudnn is available\n",
    "if (torch.backends.cudnn.version() != None and USE_CUDA == True):\n",
    "    discriminator_criterion = discriminator_criterion.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Adam optimization as proposed in [11] and set the learning-rate  l=0.00001. Each mini-batch step the optimizer will update the generator parameters $\\theta$ as well as the discriminator parameters $\\phi$ according to degree of discrimination error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# define generator and discriminator learning rate\n",
    "learning_rate_dis_z = 1e-5\n",
    "\n",
    "# define generator and discriminator optimization strategy\n",
    "discriminator_optimizer = optim.Adam(discriminator_train.parameters(), lr=learning_rate_dis_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have successfully implemented and defined the three AAE building blocks let's take some time to review the `encoder-generator`, `decoder` and `discriminator` model definition as well as the loss. Please, read the above code and comments carefully and don't hesitate to let us know any questions you might have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the Imposed Latent Prior Distribution $p(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to partition the journal entry representations learned by the AAE, we sample from a prior distribution $p(z)$ comprised of a mixture of $\\tau$ multivariate isotropic Gaussians $\\mathcal{N}(\\mu,\\mathcal{I})$, where $\\mu \\in \\mathcal{R}^{2}$. In the following example we create a prior distribution $p(z)$ consisting $\\tau=5$ isotropic Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# define the number of gaussians\n",
    "tau = 5 \n",
    "\n",
    "# define radius of each gaussian\n",
    "radius = 0.8\n",
    "\n",
    "# define the sigma of each gaussian\n",
    "sigma = 0.01\n",
    "\n",
    "# define the dimensionality of each gaussian\n",
    "dim = 2\n",
    "\n",
    "# determine x and y coordinates of the target mixture of gaussians\n",
    "x_centroid = (radius * np.sin(np.linspace(0, 2 * np.pi, tau, endpoint=False)) + 1) / 2\n",
    "y_centroid = (radius * np.cos(np.linspace(0, 2 * np.pi, tau, endpoint=False)) + 1) / 2\n",
    "\n",
    "# determine each gaussians mean (centroid) and standard deviation\n",
    "mu_gauss = np.vstack([x_centroid, y_centroid]).T\n",
    "\n",
    "# determine the number of samples to be created per gaussian\n",
    "samples_per_gaussian = 100000\n",
    "\n",
    "# iterate over the number of distinct gaussians\n",
    "for i, mu in enumerate(mu_gauss):\n",
    "\n",
    "    # case: first gaussian\n",
    "    if i == 0:\n",
    "\n",
    "        # randomly sample from gaussion distribution \n",
    "        z_continous_samples_all = np.random.normal(mu, sigma, size=(samples_per_gaussian, dim))\n",
    "\n",
    "    # case: non-first gaussian\n",
    "    else:\n",
    "\n",
    "        # randomly sample from gaussian distribution\n",
    "        z_continous_samples = np.random.normal(mu, sigma, size=(samples_per_gaussian, dim))\n",
    "\n",
    "        # collect and stack new samples\n",
    "        z_continous_samples_all = np.vstack([z_continous_samples_all, z_continous_samples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visually inspect the generated prior distribution $p(z)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# init the plot\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot reconstruction error scatter plot\n",
    "ax.scatter(z_continous_samples_all[:, 0], z_continous_samples_all[:, 1], c='C0', marker=\"o\", edgecolors='w', linewidth=0.5) \n",
    "ax.set_xlabel('$z_1$')\n",
    "ax.set_ylabel('$z_2$')\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Prior Latent Space Distribution $p(z)$');"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Training the Adversarial Autoencoder Neural Network (AAE) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train our deep adversarial autoencoder neural network (as implemented in section 3. of the lab) using the encoded transactional data (created in section 2. of the lab) as well as the prior distribution (created in section 4. of the lab). More specifically, we will have a detailed look into the distinct training steps as well as how to monitor the training progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Network Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now start to train a corresponding model for **100 epochs** and a **mini-batch size of 128** journal entries per batch. This implies that the whole dataset will be fed to the AENN 5 times in chunks of 128 journal entries yielding to 4,165 mini-batches (533,009 journal entries / 128 journal entries per mini-batch) per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# specify training parameters\n",
    "num_epochs = 100\n",
    "start_epoch = 0\n",
    "mini_batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training phase, we will fetch the individual mini-batches of the entire population of journal entries. To achieve this, we will use PyTorch's `DataLoader` that provides single- or multi-process iterators over a given dataset to load one mini-batch at a time. By enabling `shuffle=True` the data will be reshuffled at every epoch prior to feeding it to the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# convert pre-processed data to pytorch tensor\n",
    "torch_dataset = torch.from_numpy(train.values).float()\n",
    "\n",
    "# convert to pytorch tensor - none cuda enabled\n",
    "dataloader = DataLoader(torch_dataset, batch_size=mini_batch_size, shuffle=True, num_workers=0)\n",
    "# note: we set num_workers to zero to retrieve deterministic results\n",
    "\n",
    "# determine if CUDA is available at compute node\n",
    "if (torch.backends.cudnn.version() != None) and (USE_CUDA == True):\n",
    "    dataloader = DataLoader(torch_dataset.cuda(), batch_size=mini_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we start training the model. The training procedure of each mini-batch is performed in two phases, namely a (1) reconstruction phase and (2) regularization phase. The distinct training steps conducted during the reconstruction phase are the following:\n",
    "\n",
    ">1. do a journal entry batch forward pass through the encoder-decoder networks,\n",
    ">2. compute the combined reconstruction loss $\\mathcal{L}_{\\theta}^{REC}(x^{i};\\hat{x}^{i})$, \n",
    ">3. do a backward pass through the encoder-decoder part, and,\n",
    ">4. update the parameters of the encoder $q_\\theta(\\cdot)$ and decoder $p_\\theta(\\cdot)$ networks.\n",
    "\n",
    "The distinct training steps conducted during the regularization phase are the following (discriminator training):\n",
    "\n",
    ">1. do a journal entry batch forward pass through the generator network to obtain representation $z^{i}_{fake} \\sim q_\\theta(\\cdot)$,\n",
    ">2. sample representation from the imposed mixture of Gaussians prior distribution $z^{i}_{real} \\sim p(\\cdot)$,\n",
    ">3. compute the discrimination loss $\\mathcal{L}_{\\theta}^{DIS}(z^{i}_{fake}; z^{i}_{real})$, and,\n",
    ">4. update the parameters of the discriminator $d_\\phi(\\cdot)$ network.\n",
    "\n",
    "The distinct training steps conducted during the regularization phase are the following (generator training):\n",
    "\n",
    ">1. do a journal entry batch forward pass through the generator network to obtain representation $z^{i}_{fake} \\sim q_\\theta(\\cdot)$,\n",
    ">2. compute the generation loss $\\mathcal{L}_{\\theta}^{GEN}(z^{i}_{fake})$, and,\n",
    ">3. update the parameters of the generator $q_\\theta(\\cdot)$ network.\n",
    "\n",
    "To ensure learning while training our AENN model we will monitor whether the distinct losses decrease with progressing training. Therefore, we obtain and evaluate the reconstruction loss $\\mathcal{L}_{\\theta}^{REC}$, the discrimination loss $\\mathcal{L}_{\\theta}^{DIS}$, as well as, the generation loss $\\mathcal{L}_{\\theta}^{GEN}$ of the entire dataset after each training epoch. Based on this evaluation we conclude on the training progress and whether the loss is converging (indicating that the learned model might not improve any further).\n",
    "\n",
    "In addition, after each training epoch we want to save a checkpoint for both the actual `encoder`, `decoder` and `discriminator` model. The saved model checkpoints contain a snapshot of the trained model parameter values upon completion of a training epoch. In general, it is good practice, to save checkpoints at regular intervals during training. In case your system crashes during training you are able continue from the last checkpoint rather than start over from scratch.\n",
    "\n",
    ">- `torch.save()`: saves a checkpoint of the actual encoder and decoder model parameter values to disc.\n",
    "    \n",
    "Initialize the per epoch collected model losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# init collection of training losses\n",
    "epoch_reconstruction_losses = []\n",
    "epoch_discriminator_losses = []\n",
    "epoch_generator_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the verbose step size of the adversarial autoencoder training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "mini_batch_verbose_step = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now start the adversarial autoencoder network training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_enc = \"../checkpoint/checkpoint_enc.pth.tar\"\n",
    "check_dec = \"../checkpoint/checkpoint_dec.pth.tar\"\n",
    "check_dis = \"../checkpoint/checkpoint_dis.pth.tar\"\n",
    "check_rec_loss = \"../checkpoint/checkpoint_rec_loss.csv\"\n",
    "check_dis_loss = \"../checkpoint/checkpoint_dis_loss.csv\"\n",
    "check_gen_loss = \"../checkpoint/checkpoint_gen_loss.csv\"\n",
    "\n",
    "if os.path.isfile(check_enc):\n",
    "    log()\n",
    "    print(\"Loading encoder checkpoint . . .\")\n",
    "    checkpoint = torch.load(check_enc,\n",
    "                           map_location=torch.device('cpu'))\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    encoder_train.load_state_dict(checkpoint['state_dict'])\n",
    "    print(f\"Loaded encoder checkpoint trained for {start_epoch:04} / {num_epochs:04}\")\n",
    "\n",
    "if os.path.isfile(check_dec):\n",
    "    log()\n",
    "    print(\"Loading decoder checkpoint . . .\")\n",
    "    checkpoint = torch.load(check_dec,\n",
    "                           map_location=torch.device('cpu'))\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    decoder_train.load_state_dict(checkpoint['state_dict'])\n",
    "    print(f\"Loaded decoder checkpoint trained for {start_epoch:04} / {num_epochs:04}\")\n",
    "    \n",
    "if os.path.isfile(check_dis):\n",
    "    log()\n",
    "    print(\"Loading discriminator checkpoint . . .\")\n",
    "    checkpoint = torch.load(check_dis,\n",
    "                           map_location=torch.device('cpu'))\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    discriminator_train.load_state_dict(checkpoint['state_dict'])\n",
    "    print(f\"Loaded discriminator checkpoint trained for {start_epoch:04} / {num_epochs:04}\")\n",
    "    \n",
    "if os.path.isfile(check_rec_loss):\n",
    "    log()\n",
    "    print(\"Loading reconstruction loss checkpoint . . .\")\n",
    "    with open(check_rec_loss, \"r\") as f:\n",
    "        reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "        tmp_list = list(reader)\n",
    "        for i in tmp_list:\n",
    "            epoch_reconstruction_losses.extend(i)\n",
    "    print(f\"Loaded reconstruction loss checkpoint for {(len(epoch_reconstruction_losses)):04} / {num_epochs:04}\")\n",
    "\n",
    "if os.path.isfile(check_dis_loss):\n",
    "    log()\n",
    "    print(\"Loading discriminator loss checkpoint . . .\")\n",
    "    with open(check_dis_loss, \"r\") as f:\n",
    "        reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "        tmp_list = list(reader)\n",
    "        for i in tmp_list:\n",
    "            epoch_discriminator_losses.extend(i)\n",
    "    print(f\"Loaded discriminator loss checkpoint for {(len(epoch_discriminator_losses)):04} / {num_epochs:04}\")\n",
    "\n",
    "if os.path.isfile(check_gen_loss):\n",
    "    log()\n",
    "    print(\"Loading generator loss checkpoint . . .\")\n",
    "    with open(check_gen_loss, \"r\") as f:\n",
    "        reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "        tmp_list = list(reader)\n",
    "        for i in tmp_list:\n",
    "            epoch_generator_losses.extend(i)\n",
    "    print(f\"Loaded generator loss checkpoint for {(len(epoch_generator_losses)):04} / {num_epochs:04}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize training adversarial autoencoder model\n",
    "num_epochs = num_epochs - start_epoch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # init mini batch counter\n",
    "    mini_batch_count = 0\n",
    "    \n",
    "    # init epoch training losses\n",
    "    batch_reconstruction_losses = 0.0\n",
    "    batch_discriminator_losses = 0.0\n",
    "    batch_generator_losses = 0.0\n",
    "\n",
    "    # set networks in training mode (apply dropout when needed)\n",
    "    encoder_train.train()\n",
    "    decoder_train.train()\n",
    "    discriminator_train.train()\n",
    "    \n",
    "    # start timer\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # iterate over epoch mini batches\n",
    "    for mini_batch_data in dataloader:\n",
    "\n",
    "        # increase mini batch counter\n",
    "        mini_batch_count += 1\n",
    "        \n",
    "        # convert mini batch to torch variable\n",
    "        mini_batch_torch = torch.FloatTensor(mini_batch_data)\n",
    "        \n",
    "        # reset the networks gradients\n",
    "        encoder_train.zero_grad()\n",
    "        decoder_train.zero_grad()\n",
    "        discriminator_train.zero_grad()\n",
    "\n",
    "        # =================== reconstruction phase =====================\n",
    "        \n",
    "        # run autoencoder encoding - decoding\n",
    "        z_sample = encoder_train(mini_batch_torch)\n",
    "        mini_batch_reconstruction = decoder_train(z_sample)\n",
    "\n",
    "        # split input date to numerical and categorical part\n",
    "        batch_cat = mini_batch_torch[:, :ori_dataset_categ_transformed.shape[1]]\n",
    "        batch_num = mini_batch_torch[:, ori_dataset_categ_transformed.shape[1]:]\n",
    "        \n",
    "        # split reconstruction to numerical and categorical part\n",
    "        rec_batch_cat = mini_batch_reconstruction[:, :ori_dataset_categ_transformed.shape[1]]\n",
    "        rec_batch_num = mini_batch_reconstruction[:, ori_dataset_categ_transformed.shape[1]:]\n",
    "\n",
    "        # backward pass + gradients update\n",
    "        rec_error_cat = reconstruction_criterion_categorical(input=rec_batch_cat, target=batch_cat)  # one-hot attr error\n",
    "        rec_error_num = reconstruction_criterion_numeric(input=rec_batch_num, target=batch_num)  # numeric attr error\n",
    "\n",
    "        # combine both reconstruction errors\n",
    "        reconstruction_loss = rec_error_cat + rec_error_num\n",
    "        \n",
    "        # run backward pass - determine gradients\n",
    "        reconstruction_loss.backward()\n",
    "        \n",
    "        # collect batch reconstruction loss\n",
    "        batch_reconstruction_losses += reconstruction_loss.item()\n",
    "        \n",
    "        # update network parameter - decoder and encoder\n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "\n",
    "        # =================== regularization phase =====================\n",
    "        # =================== discriminator training ===================\n",
    "\n",
    "        # set discriminator in evaluation mode\n",
    "        discriminator_train.eval()\n",
    "\n",
    "        # generate target latent space data\n",
    "        z_target_batch = z_continous_samples_all[random.sample(range(0, z_continous_samples_all.shape[0]), mini_batch_size),:]\n",
    "\n",
    "        # convert to torch tensor\n",
    "        z_target_batch = torch.FloatTensor(z_target_batch)\n",
    "\n",
    "        # determine mini batch sample generated by the encoder -> fake gaussian sample\n",
    "        z_fake_gauss = encoder_train(mini_batch_torch)\n",
    "\n",
    "        # determine discriminator classification of both samples\n",
    "        d_real_gauss = discriminator_train(z_target_batch) # real sampled gaussian \n",
    "        d_fake_gauss = discriminator_train(z_fake_gauss) # fake created gaussian\n",
    "\n",
    "        # determine discriminator classification target variables\n",
    "        d_real_gauss_target = torch.FloatTensor(torch.ones(d_real_gauss.shape)) # real -> 1\n",
    "        d_fake_gauss_target = torch.FloatTensor(torch.zeros(d_fake_gauss.shape)) # fake -> 0\n",
    "\n",
    "        # determine individual discrimination losses\n",
    "        discriminator_loss_real = discriminator_criterion(target=d_real_gauss_target, input=d_real_gauss) # real loss\n",
    "        discriminator_loss_fake = discriminator_criterion(target=d_fake_gauss_target, input=d_fake_gauss) # fake loss\n",
    "        \n",
    "        # add real loss and fake loss\n",
    "        discriminator_loss = discriminator_loss_fake + discriminator_loss_real\n",
    "\n",
    "        # run backward through the discriminator network\n",
    "        discriminator_loss.backward()\n",
    "        \n",
    "        # collect discriminator loss\n",
    "        batch_discriminator_losses += discriminator_loss.item()\n",
    "\n",
    "        # update network the discriminator network parameters\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        # reset the networks gradients\n",
    "        encoder_train.zero_grad()\n",
    "        decoder_train.zero_grad()\n",
    "        discriminator_train.zero_grad()\n",
    "\n",
    "        # =================== regularization phase =====================\n",
    "        # =================== generator training =======================\n",
    "\n",
    "        # set encoder / generator in training mode\n",
    "        encoder_train.train()\n",
    "        \n",
    "        # reset the encoder / generator networks gradients\n",
    "        encoder_train.zero_grad()\n",
    "\n",
    "        # determine fake gaussian sample generated by the encoder / generator\n",
    "        z_fake_gauss = encoder_train(mini_batch_torch)\n",
    "\n",
    "        # determine discriminator classification of fake gaussian sample\n",
    "        d_fake_gauss = discriminator_train(z_fake_gauss)\n",
    "\n",
    "        # determine discriminator classification target variables\n",
    "        d_fake_gauss_target = torch.FloatTensor(torch.ones(d_fake_gauss.shape)) # fake -> 1\n",
    "\n",
    "        # determine discrimination loss of fake gaussian sample\n",
    "        generator_loss = discriminator_criterion(target=d_fake_gauss_target, input=d_fake_gauss)\n",
    "        \n",
    "        # collect generator loss\n",
    "        batch_generator_losses += generator_loss.item()\n",
    "\n",
    "        # run backward pass - determine gradients\n",
    "        generator_loss.backward()\n",
    "\n",
    "        # update network paramaters - encoder / generatorc\n",
    "        encoder_optimizer.step()\n",
    "\n",
    "        # reset the networks gradients\n",
    "        encoder_train.zero_grad()\n",
    "        decoder_train.zero_grad()\n",
    "        discriminator_train.zero_grad()\n",
    "\n",
    "    # collect epoch training losses - reconstruction loss\n",
    "    epoch_reconstruction_loss = batch_reconstruction_losses / mini_batch_count\n",
    "    epoch_reconstruction_losses.extend([epoch_reconstruction_loss])\n",
    "    \n",
    "    # collect epoch training losses - discriminator loss\n",
    "    epoch_discriminator_loss = batch_discriminator_losses / mini_batch_count\n",
    "    epoch_discriminator_losses.extend([epoch_discriminator_loss])\n",
    "    \n",
    "    # collect epoch training losses - generator loss\n",
    "    epoch_generator_loss = batch_generator_losses / mini_batch_count\n",
    "    epoch_generator_losses.extend([epoch_generator_loss])\n",
    "    \n",
    "    # print epoch reconstruction loss\n",
    "    log()\n",
    "    print(f'Epoch {(start_epoch + epoch + 1):04} / {(num_epochs + start_epoch):04}:{nl}\\\n",
    "    Reconstruction Loss: {epoch_reconstruction_loss:.4f}{nl}\\\n",
    "    Discriminator Loss: {epoch_discriminator_loss:.4f}{nl}\\\n",
    "    Generator Loss: {epoch_generator_loss:.4f}{nl}\\\n",
    "    Time Taken: {datetime.now() - start_time}{nl}\\\n",
    "    {nl}\\\n",
    "    Saving checkpoint . . .')\n",
    "    \n",
    "    # =================== save model snapshots to disk ============================\n",
    "    \n",
    "    # save trained encoder model file to disk\n",
    "    encoder_model_name = str(f\"x{mini_batch_size}_ep_{(start_epoch + epoch + 1):04}_encoder_model.pth\")\n",
    "    torch.save(encoder_train.state_dict(), os.path.join(\"../models\", encoder_model_name))\n",
    "    torch.save({\n",
    "        'epoch': start_epoch + epoch + 1,\n",
    "        'state_dict': encoder_train.state_dict()\n",
    "    }, check_enc)\n",
    "\n",
    "    # save trained decoder model file to disk\n",
    "    decoder_model_name = str(f\"x{mini_batch_size}_ep_{(start_epoch + epoch + 1):04}_decoder_model.pth\")\n",
    "    torch.save(decoder_train.state_dict(), os.path.join(\"../models\", decoder_model_name))\n",
    "    torch.save({\n",
    "        'epoch': start_epoch + epoch + 1,\n",
    "        'state_dict': decoder_train.state_dict()\n",
    "    }, check_dec)\n",
    "    \n",
    "    # save trained discriminator model file to disk\n",
    "    decoder_model_name = str(f\"x{mini_batch_size}_ep_{(start_epoch + epoch + 1):04}_discriminator_model.pth\")\n",
    "    torch.save(discriminator_train.state_dict(), os.path.join(\"../models\", decoder_model_name))\n",
    "    torch.save({\n",
    "        'epoch': start_epoch + epoch + 1,\n",
    "        'state_dict': discriminator_train.state_dict()\n",
    "    }, check_dis)\n",
    "    \n",
    "    with open(check_rec_loss, \"w\") as output:\n",
    "        writer = csv.writer(output, lineterminator='\\n')\n",
    "        for val in epoch_reconstruction_losses:\n",
    "            writer.writerow([val])\n",
    "    \n",
    "    with open(check_dis_loss, \"w\") as output:\n",
    "        writer = csv.writer(output, lineterminator='\\n')\n",
    "        for val in epoch_discriminator_losses:\n",
    "            writer.writerow([val])\n",
    "            \n",
    "    with open(check_gen_loss, \"w\") as output:\n",
    "        writer = csv.writer(output, lineterminator='\\n')\n",
    "        for val in epoch_generator_losses:\n",
    "            writer.writerow([val])\n",
    "    \n",
    "    early_stopping(epoch_reconstruction_loss, encoder_train)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    \n",
    "    #saved\n",
    "    print(f\"{nl}\\\n",
    "    Saved{nl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now evaluate the magnitude of the distinct losses with progressing training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log()\n",
    "# # plot the reconstruction loss per training epoch\n",
    "# plt.plot(range(1, len(epoch_reconstruction_losses)+1), epoch_reconstruction_losses)\n",
    "\n",
    "# # set plot title\n",
    "# plt.title('AAE training performance')\n",
    "\n",
    "# # set plot axis labels\n",
    "# plt.xlabel('training epochs')\n",
    "# plt.ylabel('reconstruction loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log()\n",
    "# # plot the discriminator loss per training epoch\n",
    "# plt.plot(range(0, len(epoch_discriminator_losses)), epoch_discriminator_losses)\n",
    "\n",
    "# # set plot title\n",
    "# plt.title('AAE training performance')\n",
    "\n",
    "# # set plot axis labels\n",
    "# plt.xlabel('training epochs')\n",
    "# plt.ylabel('discrimination loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log()\n",
    "# # plot the generator loss per training epoch\n",
    "# plt.plot(range(0, len(epoch_generator_losses)), epoch_generator_losses)\n",
    "\n",
    "# # set plot title\n",
    "# plt.title('AAE training performance')\n",
    "\n",
    "# # set plot axis labels\n",
    "# plt.xlabel('training epochs')\n",
    "# plt.ylabel('generation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the reconstruction loss change as we progress in training our model? After 5 epochs, we can observe that our reconstruction loss already went down significantly and starts to converge nicely. This indicates that our network did a pretty good job in learning the structure and attributes of the journal entries.\n",
    "\n",
    "But, from the plot we also observe that the model could probably be trained a couple more epochs as the trend of the reconstruction error still decreases for the last few epochs. In order to save time, we will continue the lab using a pre-trained model already trained by 20 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Autoencoder Neural Network (AENN) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order, to detect interpretable accounting anomalies in real-world ERP datasets we propose a novel anomaly score utilizing the introduced AAE architecture. The score builds on the regularisation applied throughout the AAE training process, namely the reconstruction error loss and the adversarial loss, described in the following.\n",
    "\n",
    "As the training of such model is computationally expensive, within the scope of this notebook we provide the pretrained model (400 training epochs) that can be used to assess the proposed anomaly score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# restore pretrained model checkpoint\n",
    "encoder_model_name = '../models/x64_ep_0433_encoder_model.pth'\n",
    "decoder_model_name = '../models/x64_ep_0433_decoder_model.pth'\n",
    "# encoder_model_name = '../models/cx128_ep_401_encoder_model.pth'\n",
    "# decoder_model_name = '../models/cx128_ep_401_decoder_model.pth'\n",
    "\n",
    "# encoder_train = Encoder(input_size=ori_subset_transformed.shape[1], hidden_size=[256, 64, 16, 4, 2])\n",
    "# encoder_train\n",
    "\n",
    "# # Read stored model from the remote location\n",
    "# encoder_bytes = urllib.request.urlopen(encoder_model_name)\n",
    "# decoder_bytes = urllib.request.urlopen(decoder_model_name)\n",
    "\n",
    "# # Load tensor from io.BytesIO object\n",
    "# encoder_buffer = io.BytesIO(encoder_bytes.read())\n",
    "# decoder_buffer = io.BytesIO(decoder_bytes.read())\n",
    "\n",
    "# init training network classes / architectures\n",
    "encoder_eval = Encoder(input_size=ori_subset_transformed.shape[1], hidden_size=[256, 64, 16, 4, 2])\n",
    "decoder_eval = Decoder(output_size=ori_subset_transformed.shape[1], hidden_size=[2, 4, 16, 64, 256])\n",
    "\n",
    "# push to cuda if cudnn is available\n",
    "if (torch.backends.cudnn.version() != None) and (USE_CUDA == True):\n",
    "    encoder_eval = encoder_eval.cuda()\n",
    "    decoder_eval = decoder_eval.cuda()\n",
    "\n",
    "# load trained models\n",
    "encoder_eval.load_state_dict(torch.load(encoder_model_name, map_location=torch.device('cpu')))\n",
    "decoder_eval.load_state_dict(torch.load(decoder_model_name, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also specify a dataloader that provides the ability to evaluate the journal entrie in an \"unshuffled\" batch-wise manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# convert pre-processed data to pytorch tensor\n",
    "torch_dataset = torch.from_numpy(ori_subset_transformed.values).float()\n",
    "\n",
    "# convert to pytorch tensor - none cuda enabled\n",
    "dataloader_eval = DataLoader(torch_dataset, batch_size=mini_batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# determine if CUDA is available at the compute node\n",
    "if (torch.backends.cudnn.version() != None) and (USE_CUDA == True):\n",
    "    \n",
    "    # push dataloader to CUDA\n",
    "    dataloader_eval = DataLoader(torch_dataset.cuda(), batch_size=mini_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Latent Space Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the feed forward pass through the pretrained encoder model and collect learned representations of all journal entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# set networks in evaluation mode (don't apply dropout)\n",
    "encoder_eval.eval()\n",
    "decoder_eval.eval()\n",
    "\n",
    "# init batch count\n",
    "batch_count = 0\n",
    "\n",
    "# iterate over epoch mini batches\n",
    "for enc_transactions_batch in dataloader_eval:\n",
    "\n",
    "    # determine latent space representation of all transactions\n",
    "    z_enc_transactions_batch = encoder_eval(enc_transactions_batch)\n",
    "    \n",
    "    # case: initial batch \n",
    "    if batch_count == 0:\n",
    "\n",
    "        # collect reconstruction errors of batch\n",
    "        z_enc_transactions_all = z_enc_transactions_batch\n",
    "      \n",
    "    # case: non-initial batch\n",
    "    else:\n",
    "      \n",
    "        # collect reconstruction errors of batch\n",
    "        z_enc_transactions_all = torch.cat((z_enc_transactions_all, z_enc_transactions_batch), dim=0)\n",
    "    \n",
    "    # increase batch count\n",
    "    batch_count += 1\n",
    "\n",
    "# convert to numpy array\n",
    "z_enc_transactions_all = z_enc_transactions_all.cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visually inspect the learned latent space representation obtained of each journal entry $x^{i}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(z_enc_transactions_all[label == 'regular'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# prepare plot\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# obtain regular transactions as well as global and local anomalies\n",
    "regular_data = z_enc_transactions_all[label == 'regular']\n",
    "global_outliers = z_enc_transactions_all[label == 'global']\n",
    "local_outliers = z_enc_transactions_all[label == 'local']\n",
    "\n",
    "# plot reconstruction error scatter plot\n",
    "ax.scatter(regular_data[:, 0], regular_data[:, 1], c='C0', marker=\"o\", label='regular', edgecolors='w', linewidth=0.5) # plot regular transactions\n",
    "ax.scatter(global_outliers[:, 0], global_outliers[:, 1], c='C1', marker=\"x\", label='global', edgecolors='w', s=60) # plot global outliers\n",
    "ax.scatter(local_outliers[:, 0], local_outliers[:, 1], c='C3', marker=\"x\", label='local', edgecolors='w', s=60) # plot local outliers\n",
    "\n",
    "# add plot legend of transaction classes\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Normalized Mode Divergence (MD) of Each Journal Entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Journal entries that exhibit anomalous attribute values (global anomalies) result in an increased divergence from the imposed multi-modal prior, e.g., as in this work the divergence to the modes of an imposed mixture of multivariate isotropic Gaussians $\\mathcal{N}(\\mu,\\mathcal{I})$, where $\\mu \\in \\mathcal{R}^m$ defines the $\\tau$ modes of the distinct Gaussians denoted by $\\mu =\\{\\mu^1 \\ldots \\mu^\\tau \\}$. Throughout the AAE training, the entries will be \"pushed\" towards the high probability density regions of the prior by the regularization. In order to be able to discriminate between the imposed prior and the learned aggregated posterior the AAE aims to keep the majority of the entries within the high-density regions (modes) of the prior. In contrast, representations that correspond to rare or anomalous journal entries will tend to differ from the imposed modes and be placed in the priors low-density regions. We use this characteristic and obtain an entry's $x^{i}$ mode divergence $D$ as the Euclidean distance of the entry's learned representation $z^i$ to its closest mode $\\mu^\\tau$. Formally, we derive the mode divergence as denoted by $D_{\\theta^*}^{\\tau}(z^{i};\\mu) = \\min\\limits_{\\tau} \\lVert z^i-\\mu^\\tau \\rVert^2 $ under optimal model parameters $\\theta^*$. Finally, we calculate the normalized mode divergence $MD$ as expressed by:\n",
    "\n",
    "\\begin{equation}\n",
    "MD_{\\theta^*}^{\\tau}(x^{i}) = \\frac{D_{\\theta^*}^{\\tau}(z^i;\\mu) - D_{\\theta^*, min}^{\\tau}}{D_{\\theta^*, max}^{\\tau} - D_{\\theta^*, min}^{\\tau}},\n",
    "\\end{equation}\n",
    "\n",
    "where $D_{min}$ and $D_{max}$ denotes the min- and max-values of the obtained mode divergences given by $D_{\\theta^*}$ and closest mode $\\tau$.\n",
    "\n",
    "Obtain the mode assignment and mode divergence of each journal entry $x^{i}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# determine distance to each mode\n",
    "distances = np.apply_along_axis(func1d=compute_euclid_distance, axis=1, arr=z_enc_transactions_all, y=mu_gauss)\n",
    "\n",
    "# determine mode divergence\n",
    "mode_divergence = np.min(distances, axis=1)\n",
    "\n",
    "# determine min-mode id\n",
    "cluster_ids = np.argmin(distances, axis=1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the obtained mode divergenes $D_{\\theta^*}^{\\tau}$ of each mode $\\tau$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# normalize the mode divergences of each mode\n",
    "\n",
    "# prepare empty arrays of the same shape and dtype\n",
    "mode_divergence_all_scaled = np.asarray(mode_divergence)\n",
    "\n",
    "# iterate over the cluster modes\n",
    "for cluster_id in np.unique(cluster_ids).tolist():\n",
    "  \n",
    "    # determine journal entries of current mode\n",
    "    mask = cluster_ids == cluster_id\n",
    "\n",
    "    # normalize mode journal entries mode divergence to the range [0,1]\n",
    "    mode_divergence_all_scaled[mask] = (mode_divergence[mask] - mode_divergence[mask].min()) / (mode_divergence[mask].ptp())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visually inspect the obtained mode divergence obtained of each journal entry $x^{i}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# collect anomaly score, labels and cluster assignments\n",
    "plot_data = pd.concat([pd.Series(mode_divergence_all_scaled, name='mode_divergence'), \n",
    "                       pd.Series(label, name='label'),                        \n",
    "                       pd.Series(cluster_ids, name='cluster_id')],\n",
    "                     axis=1)\n",
    "\n",
    "num_clusters = len(np.unique(cluster_ids))\n",
    "# init sub-plots based on the number of modes\n",
    "fig, axes = plt.subplots(1, num_clusters, sharey=True, figsize=(14, 10))\n",
    "\n",
    "\n",
    "# iterate over distinct modes\n",
    "for mode in range(0, num_clusters):\n",
    "\n",
    "    plot_data = plot_data.sample(frac=1.0)\n",
    "    # collect features of current mode\n",
    "    z_mode = plot_data[plot_data['cluster_id'] == mode]\n",
    "\n",
    "    regular_data = z_mode[z_mode['label'] == 'regular']\n",
    "    global_outliers = z_mode[z_mode['label'] == 'global']\n",
    "    local_outliers = z_mode[z_mode['label'] == 'local']\n",
    "\n",
    "    # create train scatter plot of regular samples\n",
    "    axes[mode].scatter(regular_data.index, regular_data['mode_divergence'],\n",
    "                       c='C0', marker='o', s=30, linewidth=0.3, label='regular', edgecolors='w')\n",
    "    \n",
    "    # create train scatter plot of global anomalies\n",
    "    axes[mode].scatter(global_outliers.index, global_outliers['mode_divergence'],\n",
    "                               c='C1', marker='x', s=120, linewidth=3, label='global', edgecolors='w')\n",
    "    # create train scatter plot of local anomalies\n",
    "    axes[mode].scatter(local_outliers.index, local_outliers['mode_divergence'],\n",
    "                               c='C3', marker='x', s=120, linewidth=3, label='local', edgecolors='w')\n",
    "\n",
    "    # set axis labels\n",
    "    xlabel = '$\\\\tau={}$' + str(mode+1) if mode == 0 else str(mode+1)\n",
    "    axes[mode].set_xlabel(xlabel, fontsize=24)\n",
    "\n",
    "    # set axis limits\n",
    "    axes[mode].set_ylim([0.0, 1.1])\n",
    "\n",
    "    axes[mode].set_xticks([int(plot_data.shape[0]/2)])\n",
    "    axes[mode].set_xticklabels(['$x_{i}$'])\n",
    "\n",
    "# set axis labels\n",
    "axes[0].set_ylabel('mode divergence $MD$', fontsize=20)\n",
    "\n",
    "# add legend to plot\n",
    "handles, labels = axes[2].get_legend_handles_labels()\n",
    "plt.legend(handles, labels, loc='center', fontsize=20, ncol=3, borderaxespad=0.,\n",
    "           bbox_to_anchor=(-6.5, 1., 9., .1))\n",
    "\n",
    "# set grid and tight plotting layout\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Normalized Reconstruction Error (RE) of Each Journal Entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Journal entries that exhibit anomalous attribute value co-occurrences (local anomalies) tend to result in an increased reconstruction error. This is caused by the compression capability of the AAE architecture. Anomalous and therefore unique attribute co-occurrences exhibit an increased probability of getting lost in the encoders \"lossy\" compression. As a result, their low dimensional representation will overlap with regular entries in the latent space and are not reconstructed correctly by the decoder. Formally, we obtain the reconstruction error $E$ of each entry $x^i$ and its reconstruction $\\hat{x}^i$ as the squared-difference denoted by $E_{\\theta^*}^{\\tau}(x^{i};\\hat{x}^{i}) = \\frac{1}{k} \\sum_{j=1}^{k}{(x^{i}_{j} - \\hat{x}^{i}_{j})}^2$ under optimal model parameters $\\theta^*$. Finally, we calculate the normalized reconstruction error $RE$ as expressed by:\n",
    "\n",
    "\\begin{equation}\n",
    "RE_{\\theta^*}^{\\tau}(x^{i};\\hat{x}^{i}) = \\frac{E_{\\theta^*}^{\\tau}(x^i;\\hat{x}^{i}) - E_{\\theta^*, min}^{\\tau}}{E_{\\theta^*, max}^{\\tau} - E_{\\theta^*, min}^{\\tau}},\n",
    "\\end{equation}\n",
    "\n",
    "where $E_{min}$ and $E_{max}$ denotes the min- and max-values of the obtained reconstruction errors given by $E_{\\theta^*}$ and closest mode $\\tau$. Let's obtain the reconstruction error of each journal entry $x^{i}$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# define the optimization criterion / loss function\n",
    "reconstruction_criterion_categorical_eval = nn.BCEWithLogitsLoss(reduction='none')\n",
    "reconstruction_criterion_numeric_eval = nn.MSELoss(reduction='none')\n",
    "\n",
    "# push to cuda if cudnn is available\n",
    "if (torch.backends.cudnn.version() != None and USE_CUDA == True):\n",
    "    reconstruction_criterion_categorical_eval = reconstruction_criterion_categorical_eval.cuda()\n",
    "    reconstruction_criterion_numeric_eval = reconstruction_criterion_numeric_eval.cuda()\n",
    "\n",
    "# set networks in evaluation mode (don't apply dropout)\n",
    "encoder_eval.eval()\n",
    "decoder_eval.eval()\n",
    "\n",
    "# init batch count\n",
    "batch_count = 0\n",
    "\n",
    "# iterate over epoch mini batches\n",
    "for enc_transactions_batch in dataloader_eval:\n",
    "\n",
    "    # determine latent space representation of all transactions\n",
    "    z_enc_transactions_batch = encoder_eval(enc_transactions_batch)\n",
    "\n",
    "    # reconstruct input samples\n",
    "    reconstruction_batch = decoder_eval(z_enc_transactions_batch)\n",
    "\n",
    "    # split input transactions into numeric and categorical parts\n",
    "    input_cat_all = enc_transactions_batch[:, :ori_dataset_categ_transformed.shape[1]]\n",
    "    input_num_all = enc_transactions_batch[:, ori_dataset_categ_transformed.shape[1]:]\n",
    "\n",
    "    # split reconstruction into numeric and categorical parts\n",
    "    rec_cat_all = reconstruction_batch[:, :ori_dataset_categ_transformed.shape[1]]\n",
    "    rec_num_all = reconstruction_batch[:, ori_dataset_categ_transformed.shape[1]:]\n",
    "\n",
    "    # compute rec error\n",
    "    rec_error_cat_all = reconstruction_criterion_categorical_eval(input=rec_cat_all, target=input_cat_all).mean(dim=1)\n",
    "    rec_error_num_all = reconstruction_criterion_numeric_eval(input=rec_num_all, target=input_num_all).mean(dim=1)\n",
    "\n",
    "    # combine categorical and numerical errors\n",
    "    rec_error_all_batch = rec_error_cat_all + rec_error_num_all\n",
    "    \n",
    "    # case: initial batch\n",
    "    if batch_count == 0:\n",
    "    \n",
    "        # collect reconstruction errors of batch\n",
    "        rec_error_all = rec_error_all_batch\n",
    "    \n",
    "    # case: non-initial batch\n",
    "    else:\n",
    "      \n",
    "        # collect reconstruction errors of batch\n",
    "        rec_error_all = torch.cat((rec_error_all, rec_error_all_batch), dim=0)\n",
    "    \n",
    "    # increase batch count\n",
    "    batch_count += 1\n",
    "\n",
    "# convert to numpy array\n",
    "rec_error_all = rec_error_all.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the obtained reconstruction errors $E_{\\theta^*}^{\\tau}$ of each mode $\\tau$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# normalize the reconstruction errors of each mode\n",
    "\n",
    "# prepare empty arrays of the same shape and dtype\n",
    "rec_error_all_scaled = np.asarray(rec_error_all)\n",
    "\n",
    "# iterate over the cluster modes\n",
    "for cluster_id in np.unique(cluster_ids).tolist():\n",
    "  \n",
    "    # determine journal entries of current mode\n",
    "    mask = cluster_ids == cluster_id\n",
    "\n",
    "    # normalize mode journal entries reconstruction error to the range [0,1]\n",
    "    rec_error_all_scaled[mask] = (rec_error_all[mask] - rec_error_all[mask].min()) / (rec_error_all[mask].ptp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visually inspect the reconstruction error obtained for each journal entry $x^{i}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "\n",
    "# collect anomaly score, labels and cluster assignments\n",
    "plot_data = pd.concat([pd.Series(rec_error_all_scaled, name='rec_error'), \n",
    "                       pd.Series(label, name='label'),                        \n",
    "                       pd.Series(cluster_ids, name='cluster_id')],\n",
    "                     axis=1)\n",
    "\n",
    "num_clusters = len(np.unique(cluster_ids))\n",
    "# init sub-plots based on the number of modes\n",
    "fig, axes = plt.subplots(1, num_clusters, sharey=True, figsize=(14, 10))\n",
    "\n",
    "\n",
    "# iterate over distinct modes\n",
    "for mode in range(0, num_clusters):\n",
    "\n",
    "    plot_data = plot_data.sample(frac=1.0)\n",
    "    # collect features of current mode\n",
    "    z_mode = plot_data[plot_data['cluster_id'] == mode]\n",
    "\n",
    "    regular_data = z_mode[z_mode['label'] == 'regular']\n",
    "    global_outliers = z_mode[z_mode['label'] == 'global']\n",
    "    local_outliers = z_mode[z_mode['label'] == 'local']\n",
    "\n",
    "    # create train scatter plot of regular samples\n",
    "    axes[mode].scatter(regular_data.index, regular_data['rec_error'],\n",
    "                       c='C0', marker='o', s=30, linewidth=0.3, label='regular', edgecolors='w')\n",
    "    \n",
    "    # create train scatter plot of global anomalies\n",
    "    axes[mode].scatter(global_outliers.index, global_outliers['rec_error'],\n",
    "                               c='C1', marker='x', s=120, linewidth=3, label='global', edgecolors='w')\n",
    "    # create train scatter plot of local anomalies\n",
    "    axes[mode].scatter(local_outliers.index, local_outliers['rec_error'],\n",
    "                               c='C3', marker='x', s=120, linewidth=3, label='local', edgecolors='w')\n",
    "\n",
    "    # set axis labels\n",
    "    xlabel = '$\\\\tau={}$' + str(mode+1) if mode == 0 else str(mode+1)\n",
    "    axes[mode].set_xlabel(xlabel, fontsize=24)\n",
    "\n",
    "    # set axis limits\n",
    "    axes[mode].set_ylim([0.0, 1.1])\n",
    "\n",
    "    axes[mode].set_xticks([int(plot_data.shape[0]/2)])\n",
    "    axes[mode].set_xticklabels(['$x_{i}$'])\n",
    "\n",
    "# set axis labels\n",
    "axes[0].set_ylabel('reconstruction error $RE$', fontsize=20)\n",
    "\n",
    "# add legend to plot\n",
    "handles, labels = axes[2].get_legend_handles_labels()\n",
    "plt.legend(handles, labels, loc='center', fontsize=20, ncol=3, borderaxespad=0.,\n",
    "           bbox_to_anchor=(-6.5, 1., 9., .1))\n",
    "\n",
    "# set grid and tight plotting layout\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Anomaly Score $AS^{\\tau}$ of Each Journal Entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantifying both characteristics for a given journal entry, we can reasonably conclude (1) if the entry is anomalous and (2) if it was created by a \"regular\" business activity. To detect global and local accounting anomalies in real-world audit scenarios we propose to score each journal entry $x^i$ by its normalized reconstruction error $RE$ regularized and normalized mode divergence $MD$ given by:\n",
    "\n",
    "\\begin{equation}\n",
    "AS^{\\tau}(x^{i};\\hat{x}^{i}) = \\alpha \\times RE_{\\theta^*}^{\\tau}(x^{i};\\hat{x}^{i}) + (1-\\alpha) \\times MD_{\\theta^*}^{\\tau}(x^{i}),\n",
    "\\end{equation} \n",
    "\n",
    "for each individual journal entry $x^{i}$ and optimal model parameters $\\theta^*$ and closest mode $\\tau$. We introduce $\\alpha$ as a factor to balance both characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# set alpha \n",
    "alpha = 0.4\n",
    "\n",
    "# determine journal entry anomaly score\n",
    "anomaly_score = alpha * rec_error_all_scaled + (1.0 - alpha) * mode_divergence_all_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Inspection of the Obtained Anomaly Scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visually inspect the obtained anomaly scores obtained for each journal entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# collect anomaly score, labels and cluster assignments\n",
    "plot_data = pd.concat([pd.Series(anomaly_score, name='anomaly_score'), \n",
    "                       pd.Series(label, name='label'),                        \n",
    "                       pd.Series(cluster_ids, name='cluster_id')],\n",
    "                     axis=1)\n",
    "\n",
    "num_clusters = len(np.unique(cluster_ids))\n",
    "# init sub-plots based on the number of modes\n",
    "fig, axes = plt.subplots(1, num_clusters, sharey=True, figsize=(14, 10))\n",
    "\n",
    "# iterate over distinct modes\n",
    "for mode in range(0, num_clusters):\n",
    "\n",
    "    plot_data = plot_data.sample(frac=1.0)\n",
    "    # collect features of current mode\n",
    "    z_mode = plot_data[plot_data['cluster_id'] == mode]\n",
    "\n",
    "    regular_data = z_mode[z_mode['label'] == 'regular']\n",
    "    global_outliers = z_mode[z_mode['label'] == 'global']\n",
    "    local_outliers = z_mode[z_mode['label'] == 'local']\n",
    "\n",
    "    # create train scatter plot of regular samples\n",
    "    axes[mode].scatter(regular_data.index, regular_data['anomaly_score'],\n",
    "                       c='C0', marker='o', s=30, linewidth=0.3, label='regular', edgecolors='w')\n",
    "    \n",
    "    # create train scatter plot of global anomalies\n",
    "    axes[mode].scatter(global_outliers.index, global_outliers['anomaly_score'],\n",
    "                               c='C1', marker='x', s=120, linewidth=3, label='global', edgecolors='w')\n",
    "    # create train scatter plot of local anomalies\n",
    "    axes[mode].scatter(local_outliers.index, local_outliers['anomaly_score'],\n",
    "                               c='C3', marker='x', s=120, linewidth=3, label='local', edgecolors='w')\n",
    "\n",
    "    # set axis labels\n",
    "    xlabel = '$\\\\tau={}$' + str(mode+1) if mode == 0 else str(mode+1)\n",
    "    axes[mode].set_xlabel(xlabel, fontsize=24)\n",
    "\n",
    "    # set axis limits\n",
    "    axes[mode].set_ylim([0.0, 1.1])\n",
    "\n",
    "    axes[mode].set_xticks([int(plot_data.shape[0]/2)])\n",
    "    axes[mode].set_xticklabels(['$x_{i}$'])\n",
    "\n",
    "# set axis labels\n",
    "axes[0].set_ylabel('anomaly score $AS$', fontsize=20)\n",
    "\n",
    "# add legend to plot\n",
    "handles, labels = axes[2].get_legend_handles_labels()\n",
    "plt.legend(handles, labels, loc='center', fontsize=20, ncol=3, borderaxespad=0.,\n",
    "           bbox_to_anchor=(-6.5, 1., 9., .1))\n",
    "\n",
    "# set grid and tight plotting layout\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization reveals that the pre-trained model is able to reconstruct the majority of regular journal entries, while failing to do so, for the anomalous ones. As a result, the model reconstruction error can be used to distinguish both \"global\" anomalies (orange) and \"local\" anomalies (green) from the regular journal entries (blue).\n",
    "\n",
    "To further investigate our observation and confirm the initial assumption, let's have a closer look into the journal entries exhibiting an anomaly score >= 0.25 selected from the cluster mode $\\tau=3$. We assume that these journal entries correspond to the \"global\" anomalies of the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "ori_dataset['label'] = label\n",
    "ori_dataset['tau'] = cluster_ids\n",
    "\n",
    "# inspect transactions exhibiting a reconstruction error >= 0.2\n",
    "ori_dataset[((anomaly_score >= 0.456) & (cluster_ids == 2)) | ((anomaly_score >= 0.456) & (cluster_ids == 3))].label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now also have a closer look into the journal entries exhibiting anomaly score >= 0.4 selected from the cluster mode $\\tau=2$. We assume that these journal entries mostly correspond to the \"local\" anomalies of the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log()\n",
    "# inspect transactions exhibiting a anomaly_score >= 0.4 from the mode 2\n",
    "ori_dataset[(((anomaly_score >= 0.133) & (anomaly_score < 0.456)) & (cluster_ids == 2)) | (((anomaly_score >= 0.288) & (anomaly_score < 0.456)) & (cluster_ids == 3))].label.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
